"use strict";(globalThis.webpackChunkeigenbytes=globalThis.webpackChunkeigenbytes||[]).push([[8464],{8453(e,n,i){i.d(n,{R:()=>l,x:()=>d});var r=i(6540);const s={},t=r.createContext(s);function l(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(t.Provider,{value:n},e.children)}},9874(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>o,frontMatter:()=>l,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"technical-papers/continuous-authentication/04 biometric-models-part1","title":"Biometric Models: Physiological","description":"Overview","source":"@site/docs/publications/technical-papers/continuous-authentication/04 biometric-models-part1.md","sourceDirName":"technical-papers/continuous-authentication","slug":"/technical-papers/continuous-authentication/04 biometric-models-part1","permalink":"/publications/technical-papers/continuous-authentication/04 biometric-models-part1","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Biometric Models: Physiological","section":"Continuous Authentication","type":"Paper Section"},"sidebar":"sidebar","previous":{"title":"System Architecture","permalink":"/publications/technical-papers/continuous-authentication/03 system-architecture"},"next":{"title":"Biometric Models: Behavioral","permalink":"/publications/technical-papers/continuous-authentication/05 biometric-models-part2"}}');var s=i(4848),t=i(8453);const l={title:"Biometric Models: Physiological",section:"Continuous Authentication",type:"Paper Section"},d="Biometric Models - Part 1: Physiological Authentication",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Face Matcher: MobileNetV2 with Triplet Loss",id:"face-matcher-mobilenetv2-with-triplet-loss",level:2},{value:"Introduction",id:"introduction",level:3},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Why MobileNetV2?",id:"why-mobilenetv2",level:3},{value:"Loss Function: Triplet Loss",id:"loss-function-triplet-loss",level:3},{value:"Training Pipeline",id:"training-pipeline",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3},{value:"1. Verification Accuracy",id:"1-verification-accuracy",level:4},{value:"2. Training and Validation Loss",id:"2-training-and-validation-loss",level:4},{value:"3. ROC Curve Analysis",id:"3-roc-curve-analysis",level:4},{value:"4. Embedding Space Visualization (t-SNE)",id:"4-embedding-space-visualization-t-sne",level:4},{value:"5. Distance Distribution Analysis",id:"5-distance-distribution-analysis",level:4},{value:"Implementation Details",id:"implementation-details",level:3},{value:"Challenges and Limitations",id:"challenges-and-limitations",level:3},{value:"Voice Matcher: GRU with Attention Mechanism",id:"voice-matcher-gru-with-attention-mechanism",level:2},{value:"Introduction",id:"introduction-1",level:3},{value:"Model Architecture",id:"model-architecture-1",level:3},{value:"Why GRU with Attention?",id:"why-gru-with-attention",level:3},{value:"Audio Preprocessing",id:"audio-preprocessing",level:3},{value:"Attention Mechanism",id:"attention-mechanism",level:3},{value:"Training Pipeline",id:"training-pipeline-1",level:3},{value:"Training Performance",id:"training-performance",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics-1",level:3},{value:"1. Classification Accuracy",id:"1-classification-accuracy",level:4},{value:"2. Loss Curves Analysis",id:"2-loss-curves-analysis",level:4},{value:"Implementation Details",id:"implementation-details-1",level:3},{value:"Challenges and Future Improvements",id:"challenges-and-future-improvements",level:3},{value:"Performance Comparison: Pre-trained vs Custom Models",id:"performance-comparison-pre-trained-vs-custom-models",level:2},{value:"Face Recognition",id:"face-recognition",level:3},{value:"Voice Recognition",id:"voice-recognition",level:3}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"biometric-models---part-1-physiological-authentication",children:"Biometric Models - Part 1: Physiological Authentication"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This section details the implementation of physiological biometric authentication models used in the continuous authentication system. These models handle Face Recognition and Voice Recognition, providing strong identity verification when behavioral biometrics indicate elevated risk levels."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"face-matcher-mobilenetv2-with-triplet-loss",children:"Face Matcher: MobileNetV2 with Triplet Loss"}),"\n",(0,s.jsx)(n.h3,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"The Face Matcher implements a deep learning-based facial recognition system inspired by FaceNet architecture. It uses MobileNetV2 as the backbone network, optimized for computational efficiency while maintaining high accuracy. The model generates 512-dimensional facial embeddings that can be compared using Euclidean distance for identity verification."}),"\n",(0,s.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Input Image<br/>160x160x3] --\x3e B[MobileNetV2<br/>Pre-trained on ImageNet]\n    B --\x3e C[Global Average Pooling<br/>5x5x1280 \u2192 1280]\n    C --\x3e D[Dense Layer<br/>1280 \u2192 512<br/>ReLU Activation]\n    D --\x3e E[L2 Normalization<br/>Unit Hypersphere]\n    E --\x3e F[512-D Embedding]\n    \n    style A fill:#e8f4f8\n    style F fill:#a8dadc"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Architecture Components:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Layer"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Output Shape"}),(0,s.jsx)(n.th,{children:"Parameters"}),(0,s.jsx)(n.th,{children:"Purpose"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Input"}),(0,s.jsx)(n.td,{children:"Image"}),(0,s.jsx)(n.td,{children:"(160, 160, 3)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"RGB facial image"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"MobileNetV2"}),(0,s.jsx)(n.td,{children:"CNN Backbone"}),(0,s.jsx)(n.td,{children:"(5, 5, 1280)"}),(0,s.jsx)(n.td,{children:"2,257,984"}),(0,s.jsx)(n.td,{children:"Feature extraction (frozen)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GlobalAveragePooling2D"}),(0,s.jsx)(n.td,{children:"Pooling"}),(0,s.jsx)(n.td,{children:"(1280,)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Spatial aggregation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Dense"}),(0,s.jsx)(n.td,{children:"Fully Connected"}),(0,s.jsx)(n.td,{children:"(512,)"}),(0,s.jsx)(n.td,{children:"655,872"}),(0,s.jsx)(n.td,{children:"Embedding projection"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"L2 Normalization"}),(0,s.jsx)(n.td,{children:"Lambda"}),(0,s.jsx)(n.td,{children:"(512,)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Unit sphere constraint"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Total Parameters:"})," 2,913,856 (11.12 MB)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Trainable:"})," 655,872 (2.50 MB)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Non-trainable:"})," 2,257,984 (8.61 MB)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"why-mobilenetv2",children:"Why MobileNetV2?"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Design Rationale:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficiency"}),": Uses depthwise separable convolutions, reducing computational cost by 8-9x compared to standard convolutions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance"}),": Maintains accuracy while being lightweight (suitable for mobile/edge deployment)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transfer Learning"}),": Pre-trained on ImageNet provides robust feature extraction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Processing"}),": Inference time of approximately 150ms per image"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Comparison with Alternatives:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Architecture"}),(0,s.jsx)(n.th,{children:"Parameters"}),(0,s.jsx)(n.th,{children:"Inference Time"}),(0,s.jsx)(n.th,{children:"Accuracy"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ResNet-50"}),(0,s.jsx)(n.td,{children:"23.5M"}),(0,s.jsx)(n.td,{children:"280ms"}),(0,s.jsx)(n.td,{children:"95.8%"}),(0,s.jsx)(n.td,{children:"Server-side, high accuracy"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"VGG-16"}),(0,s.jsx)(n.td,{children:"138M"}),(0,s.jsx)(n.td,{children:"450ms"}),(0,s.jsx)(n.td,{children:"94.2%"}),(0,s.jsx)(n.td,{children:"Legacy systems"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"MobileNetV2"}),(0,s.jsx)(n.td,{children:"3.4M"}),(0,s.jsx)(n.td,{children:"150ms"}),(0,s.jsx)(n.td,{children:"94.5%"}),(0,s.jsx)(n.td,{children:"Edge devices, real-time"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"EfficientNet-B0"}),(0,s.jsx)(n.td,{children:"5.3M"}),(0,s.jsx)(n.td,{children:"180ms"}),(0,s.jsx)(n.td,{children:"95.1%"}),(0,s.jsx)(n.td,{children:"Balanced performance"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"loss-function-triplet-loss",children:"Loss Function: Triplet Loss"}),"\n",(0,s.jsx)(n.p,{children:"The model is trained using triplet loss, which learns to minimize the distance between an anchor and positive (same identity) while maximizing distance to negative (different identity) samples."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mathematical Formulation:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"L = max(||f(a) - f(p)||\xb2 - ||f(a) - f(n)||\xb2 + \u03b1, 0)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"f(a)"})," = Embedding of anchor image"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"f(p)"})," = Embedding of positive image (same person)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"f(n)"})," = Embedding of negative image (different person)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"\u03b1"})," = Margin parameter (typically 0.2)"]}),"\n"]}),"\n",(0,s.jsx)(n.mermaid,{value:"graph LR\n    A[Anchor<br/>Person A] --\x3e M[Model]\n    P[Positive<br/>Person A] --\x3e M\n    N[Negative<br/>Person B] --\x3e M\n    \n    M --\x3e EA[Embedding A]\n    M --\x3e EP[Embedding P]\n    M --\x3e EN[Embedding N]\n    \n    EA -.->|Minimize| EP\n    EA -.->|Maximize| EN\n    \n    style A fill:#a8dadc\n    style P fill:#a8dadc\n    style N fill:#ff6b6b"}),"\n",(0,s.jsx)(n.h3,{id:"training-pipeline",children:"Training Pipeline"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Dataset:"})," Labeled Faces in the Wild (LFW)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Images:"})," 13,000 labeled images"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Identities:"})," 5,750 individuals"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Variations:"})," Different lighting, poses, expressions"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Preprocessing Steps:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Face Detection"}),": Extract face region from image"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resize"}),": Scale to 160x160 pixels"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Normalization"}),": Scale pixel values to [0, 1]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Augmentation"}),": Random flips, rotations, crops"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Training Configuration:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Value"}),(0,s.jsx)(n.th,{children:"Purpose"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Batch Size"}),(0,s.jsx)(n.td,{children:"32 triplets"}),(0,s.jsx)(n.td,{children:"Stable gradient updates"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Epochs"}),(0,s.jsx)(n.td,{children:"100"}),(0,s.jsx)(n.td,{children:"Sufficient convergence"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Learning Rate"}),(0,s.jsx)(n.td,{children:"1e-4"}),(0,s.jsx)(n.td,{children:"Fine-tuning pre-trained model"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Optimizer"}),(0,s.jsx)(n.td,{children:"Adam"}),(0,s.jsx)(n.td,{children:"Adaptive learning"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Margin (\u03b1)"}),(0,s.jsx)(n.td,{children:"0.2"}),(0,s.jsx)(n.td,{children:"Separation threshold"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Training Callbacks:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"EarlyStopping"}),": Monitors validation loss, patience of 15 epochs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ReduceLROnPlateau"}),": Reduces learning rate by 0.5 when loss plateaus for 5 epochs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ModelCheckpoint"}),": Saves best model based on validation loss"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,s.jsx)(n.h4,{id:"1-verification-accuracy",children:"1. Verification Accuracy"}),"\n",(0,s.jsxs)(n.p,{children:["Optimal threshold determined at ",(0,s.jsx)(n.strong,{children:"0.5392"})," Euclidean distance:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training Accuracy:"})," 99.72%"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validation Accuracy:"})," 99.74%"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-training-and-validation-loss",children:"2. Training and Validation Loss"}),"\n",(0,s.jsx)(n.p,{children:"The model demonstrates effective learning with minimal overfitting:"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph LR\n    A[Epoch 1] --\x3e|Loss: 0.2045| B[Epoch 2]\n    B --\x3e|Loss: 0.2002| C[Epoch 3]\n    C --\x3e|Loss: 0.2037| D[...]\n    D --\x3e E[Epoch 8]\n    E --\x3e|Loss: 0.1966| F[Convergence]\n    \n    style A fill:#ff6b6b\n    style F fill:#a8dadc"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Observations:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Training loss decreases steadily from 0.2045 to 0.1966"}),"\n",(0,s.jsx)(n.li,{children:"Validation loss remains stable around 0.198, indicating good generalization"}),"\n",(0,s.jsx)(n.li,{children:"No significant divergence between training and validation curves"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"3-roc-curve-analysis",children:"3. ROC Curve Analysis"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Area Under Curve (AUC):"})," 0.7927"]}),"\n",(0,s.jsx)(n.p,{children:"The ROC curve demonstrates the model's ability to distinguish between matching and non-matching pairs across various thresholds. An AUC of 0.79 indicates good discriminative power, though there is room for improvement through:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Increased training data diversity"}),"\n",(0,s.jsx)(n.li,{children:"Enhanced data augmentation"}),"\n",(0,s.jsx)(n.li,{children:"Fine-tuning of network architecture"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"4-embedding-space-visualization-t-sne",children:"4. Embedding Space Visualization (t-SNE)"}),"\n",(0,s.jsx)(n.p,{children:"t-SNE visualization of the 512-dimensional embeddings reduced to 2D shows:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Clear clustering of same-identity samples"}),"\n",(0,s.jsx)(n.li,{children:"Separation between different identity clusters"}),"\n",(0,s.jsx)(n.li,{children:"Some overlap at cluster boundaries (expected for challenging cases)"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"5-distance-distribution-analysis",children:"5. Distance Distribution Analysis"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Positive Pairs (Same Identity):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Mean distance: 0.32"}),"\n",(0,s.jsx)(n.li,{children:"Standard deviation: 0.15"}),"\n",(0,s.jsx)(n.li,{children:"Distribution: Concentrated at lower distances"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Negative Pairs (Different Identity):"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Mean distance: 0.78"}),"\n",(0,s.jsx)(n.li,{children:"Standard deviation: 0.21"}),"\n",(0,s.jsx)(n.li,{children:"Distribution: Spread across higher distances"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Optimal Threshold:"})," 0.5392 provides best separation between distributions"]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-details",children:"Implementation Details"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inference Process:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant U as User\n    participant C as Camera\n    participant P as Preprocessor\n    participant M as Model\n    participant D as Decision\n    \n    U->>C: Face captured\n    C->>P: Raw image\n    P->>P: Detect face\n    P->>P: Resize to 160x160\n    P->>P: Normalize pixels\n    P->>M: Preprocessed image\n    M->>M: Generate embedding\n    M->>D: 512-D vector\n    D->>D: Compare with stored\n    D->>D: Calculate distance\n    D--\x3e>U: Match/No Match"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Verification Algorithm:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def verify_face(live_embedding, stored_embedding, threshold=0.5392):\n    """\n    Verify if two face embeddings belong to same person\n    \n    Args:\n        live_embedding: 512-D vector from current image\n        stored_embedding: 512-D vector from enrolled image\n        threshold: Maximum distance for positive match\n    \n    Returns:\n        is_match: Boolean indicating if faces match\n        distance: Euclidean distance between embeddings\n    """\n    distance = euclidean_distance(live_embedding, stored_embedding)\n    is_match = distance < threshold\n    return is_match, distance\n'})}),"\n",(0,s.jsx)(n.h3,{id:"challenges-and-limitations",children:"Challenges and Limitations"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Current Challenges:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lighting Variations"}),": Performance degrades in extreme lighting conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pose Variations"}),": Large angles (>45\xb0) reduce accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Occlusions"}),": Masks, glasses, or partial face coverage affect recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Age Progression"}),": Long-term appearance changes may require re-enrollment"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mitigation Strategies:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Data augmentation with varied lighting and poses"}),"\n",(0,s.jsx)(n.li,{children:"Multi-angle enrollment during registration"}),"\n",(0,s.jsx)(n.li,{children:"Periodic re-enrollment suggestions"}),"\n",(0,s.jsx)(n.li,{children:"Confidence scoring to handle uncertain cases"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"voice-matcher-gru-with-attention-mechanism",children:"Voice Matcher: GRU with Attention Mechanism"}),"\n",(0,s.jsx)(n.h3,{id:"introduction-1",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"The Voice Matcher implements a custom speaker recognition system using Gated Recurrent Units (GRUs) with an attention mechanism. The model processes Mel spectrograms to capture unique vocal characteristics and creates speaker-specific voiceprints for authentication."}),"\n",(0,s.jsx)(n.h3,{id:"model-architecture-1",children:"Model Architecture"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Input<br/>Mel Spectrogram<br/>128x94] --\x3e B[GRU Layer 1<br/>128 units<br/>return_sequences=True]\n    B --\x3e C[Batch Normalization]\n    C --\x3e D[Dropout 30%]\n    D --\x3e E[GRU Layer 2<br/>64 units<br/>return_sequences=True]\n    E --\x3e F[Batch Normalization]\n    F --\x3e G[Dropout 30%]\n    G --\x3e H[Attention Mechanism]\n    H --\x3e I[Residual Connection]\n    I --\x3e J[GRU Layer 3<br/>64 units]\n    J --\x3e K[Dropout 30%]\n    K --\x3e L[Dense Layer<br/>128 units<br/>ReLU]\n    L --\x3e M[Batch Normalization]\n    M --\x3e N[Dropout 30%]\n    N --\x3e O[Dense Layer<br/>64 units]\n    O --\x3e P[Output Layer<br/>SoftMax<br/>665 speakers]\n    \n    style A fill:#e8f4f8\n    style P fill:#a8dadc"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Architecture Summary:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Layer"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Output Shape"}),(0,s.jsx)(n.th,{children:"Parameters"}),(0,s.jsx)(n.th,{children:"Purpose"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Input"}),(0,s.jsx)(n.td,{children:"Mel Spectrogram"}),(0,s.jsx)(n.td,{children:"(128, 94)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Frequency-time representation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GRU-1"}),(0,s.jsx)(n.td,{children:"Recurrent"}),(0,s.jsx)(n.td,{children:"(128, 128)"}),(0,s.jsx)(n.td,{children:"86,016"}),(0,s.jsx)(n.td,{children:"Capture short-term patterns"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BatchNorm-1"}),(0,s.jsx)(n.td,{children:"Normalization"}),(0,s.jsx)(n.td,{children:"(128, 128)"}),(0,s.jsx)(n.td,{children:"512"}),(0,s.jsx)(n.td,{children:"Stabilize training"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Dropout-1"}),(0,s.jsx)(n.td,{children:"Regularization"}),(0,s.jsx)(n.td,{children:"(128, 128)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Prevent overfitting"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GRU-2"}),(0,s.jsx)(n.td,{children:"Recurrent"}),(0,s.jsx)(n.td,{children:"(128, 64)"}),(0,s.jsx)(n.td,{children:"37,248"}),(0,s.jsx)(n.td,{children:"Refine temporal features"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BatchNorm-2"}),(0,s.jsx)(n.td,{children:"Normalization"}),(0,s.jsx)(n.td,{children:"(128, 64)"}),(0,s.jsx)(n.td,{children:"256"}),(0,s.jsx)(n.td,{children:"Stabilize training"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Dropout-2"}),(0,s.jsx)(n.td,{children:"Regularization"}),(0,s.jsx)(n.td,{children:"(128, 64)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Prevent overfitting"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Attention"}),(0,s.jsx)(n.td,{children:"Mechanism"}),(0,s.jsx)(n.td,{children:"(128, 64)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Focus on key segments"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Add"}),(0,s.jsx)(n.td,{children:"Residual"}),(0,s.jsx)(n.td,{children:"(128, 64)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Skip connection"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GRU-3"}),(0,s.jsx)(n.td,{children:"Recurrent"}),(0,s.jsx)(n.td,{children:"(64,)"}),(0,s.jsx)(n.td,{children:"24,960"}),(0,s.jsx)(n.td,{children:"Final feature extraction"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Dropout-3"}),(0,s.jsx)(n.td,{children:"Regularization"}),(0,s.jsx)(n.td,{children:"(64,)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Prevent overfitting"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Dense-1"}),(0,s.jsx)(n.td,{children:"Fully Connected"}),(0,s.jsx)(n.td,{children:"(128,)"}),(0,s.jsx)(n.td,{children:"8,320"}),(0,s.jsx)(n.td,{children:"Feature refinement"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BatchNorm-3"}),(0,s.jsx)(n.td,{children:"Normalization"}),(0,s.jsx)(n.td,{children:"(128,)"}),(0,s.jsx)(n.td,{children:"512"}),(0,s.jsx)(n.td,{children:"Stabilize training"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Dropout-4"}),(0,s.jsx)(n.td,{children:"Regularization"}),(0,s.jsx)(n.td,{children:"(128,)"}),(0,s.jsx)(n.td,{children:"0"}),(0,s.jsx)(n.td,{children:"Prevent overfitting"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Dense-2"}),(0,s.jsx)(n.td,{children:"Fully Connected"}),(0,s.jsx)(n.td,{children:"(64,)"}),(0,s.jsx)(n.td,{children:"8,256"}),(0,s.jsx)(n.td,{children:"Compression"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Output"}),(0,s.jsx)(n.td,{children:"SoftMax"}),(0,s.jsx)(n.td,{children:"(665,)"}),(0,s.jsx)(n.td,{children:"43,225"}),(0,s.jsx)(n.td,{children:"Speaker classification"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Total Parameters:"})," 209,305"]}),"\n",(0,s.jsx)(n.h3,{id:"why-gru-with-attention",children:"Why GRU with Attention?"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Design Rationale:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GRU Efficiency"}),": Fewer parameters than LSTM (simpler gating mechanism)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temporal Modeling"}),": Captures sequential dependencies in voice data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Attention Mechanism"}),": Focuses on discriminative voice segments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computational Efficiency"}),": 25% faster than LSTM with comparable accuracy"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"GRU vs LSTM Comparison:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Aspect"}),(0,s.jsx)(n.th,{children:"GRU"}),(0,s.jsx)(n.th,{children:"LSTM"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Gates"}),(0,s.jsx)(n.td,{children:"2 (Reset, Update)"}),(0,s.jsx)(n.td,{children:"3 (Input, Forget, Output)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Parameters"}),(0,s.jsx)(n.td,{children:"Fewer (faster training)"}),(0,s.jsx)(n.td,{children:"More (potentially better memory)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Training Speed"}),(0,s.jsx)(n.td,{children:"25% faster"}),(0,s.jsx)(n.td,{children:"Baseline"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Memory"}),(0,s.jsx)(n.td,{children:"Simpler cell state"}),(0,s.jsx)(n.td,{children:"Separate cell and hidden state"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Performance"}),(0,s.jsx)(n.td,{children:"Comparable on voice"}),(0,s.jsx)(n.td,{children:"Slightly better on very long sequences"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"audio-preprocessing",children:"Audio Preprocessing"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Input Processing Pipeline:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph LR\n    A[Raw Audio<br/>WAV file] --\x3e B[Resample<br/>to 16kHz]\n    B --\x3e C[Segment<br/>3 seconds]\n    C --\x3e D[STFT<br/>Short-Time Fourier Transform]\n    D --\x3e E[Mel Filterbank<br/>128 filters]\n    E --\x3e F[Log Scale<br/>Decibel conversion]\n    F --\x3e G[Normalization<br/>Mean=0, Std=1]\n    G --\x3e H[Mel Spectrogram<br/>128x94]\n    \n    style A fill:#e8f4f8\n    style H fill:#a8dadc"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Preprocessing Steps:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resampling"}),": Standardize to 16 kHz sample rate"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Segmentation"}),": Fixed 3-second windows (truncate or pad)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"STFT"}),": Convert time-domain to frequency-domain"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mel Filterbank"}),": Apply 128 mel-scale filters (human perception)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Logarithmic Scaling"}),": Convert to decibels"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Normalization"}),": Zero mean, unit variance"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"attention-mechanism",children:"Attention Mechanism"}),"\n",(0,s.jsx)(n.p,{children:"The attention layer allows the model to focus on the most discriminative portions of the audio sequence."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Attention Computation:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Attention Score = softmax(Query \xb7 Key^T / \u221ad_k)\nOutput = Attention Score \xb7 Value\n"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[GRU Output Sequence<br/>128x64] --\x3e B[Query]\n    A --\x3e C[Key]\n    A --\x3e D[Value]\n    \n    B --\x3e E[Similarity Scores]\n    C --\x3e E\n    E --\x3e F[SoftMax<br/>Attention Weights]\n    F --\x3e G[Weighted Sum]\n    D --\x3e G\n    G --\x3e H[Context Vector<br/>64-D]\n    \n    style A fill:#e8f4f8\n    style H fill:#a8dadc"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Benefits:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Identifies speaker-specific voice characteristics"}),"\n",(0,s.jsx)(n.li,{children:"Handles variable-length audio inputs"}),"\n",(0,s.jsx)(n.li,{children:"Improves interpretability (which parts of audio are important)"}),"\n",(0,s.jsx)(n.li,{children:"Reduces impact of background noise"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"training-pipeline-1",children:"Training Pipeline"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Dataset:"})," Mozilla Common Voice"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speakers:"})," 665 unique speakers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Utterances:"})," Multiple recordings per speaker"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Duration:"})," 3-second segments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quality:"})," 16 kHz, mono audio"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Training Configuration:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Value"}),(0,s.jsx)(n.th,{children:"Rationale"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Batch Size"}),(0,s.jsx)(n.td,{children:"32"}),(0,s.jsx)(n.td,{children:"Memory constraints"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Epochs"}),(0,s.jsx)(n.td,{children:"100"}),(0,s.jsx)(n.td,{children:"With early stopping"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Learning Rate"}),(0,s.jsx)(n.td,{children:"1e-3"}),(0,s.jsx)(n.td,{children:"Initial rate for Adam"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Optimizer"}),(0,s.jsx)(n.td,{children:"Adam"}),(0,s.jsx)(n.td,{children:"Adaptive learning rate"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Loss Function"}),(0,s.jsx)(n.td,{children:"Sparse Categorical Crossentropy"}),(0,s.jsx)(n.td,{children:"Multi-class classification"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Validation Split"}),(0,s.jsx)(n.td,{children:"20%"}),(0,s.jsx)(n.td,{children:"Held-out evaluation"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Training Callbacks:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"EarlyStopping"}),": Patience of 20 epochs on validation loss"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ReduceLROnPlateau"}),": Factor of 0.5, patience of 5 epochs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ModelCheckpoint"}),": Save best model by validation accuracy"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"training-performance",children:"Training Performance"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Loss Progression:"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Epoch"}),(0,s.jsx)(n.th,{children:"Training Loss"}),(0,s.jsx)(n.th,{children:"Validation Loss"}),(0,s.jsx)(n.th,{children:"Training Accuracy"}),(0,s.jsx)(n.th,{children:"Validation Accuracy"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"6.3481"}),(0,s.jsx)(n.td,{children:"5.8373"}),(0,s.jsx)(n.td,{children:"1.83%"}),(0,s.jsx)(n.td,{children:"27.07%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2"}),(0,s.jsx)(n.td,{children:"5.3526"}),(0,s.jsx)(n.td,{children:"4.1138"}),(0,s.jsx)(n.td,{children:"25.08%"}),(0,s.jsx)(n.td,{children:"34.93%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3"}),(0,s.jsx)(n.td,{children:"3.9417"}),(0,s.jsx)(n.td,{children:"3.6045"}),(0,s.jsx)(n.td,{children:"34.10%"}),(0,s.jsx)(n.td,{children:"36.56%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"4"}),(0,s.jsx)(n.td,{children:"3.4869"}),(0,s.jsx)(n.td,{children:"3.4015"}),(0,s.jsx)(n.td,{children:"38.27%"}),(0,s.jsx)(n.td,{children:"41.16%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"5"}),(0,s.jsx)(n.td,{children:"3.2851"}),(0,s.jsx)(n.td,{children:"3.2485"}),(0,s.jsx)(n.td,{children:"42.61%"}),(0,s.jsx)(n.td,{children:"46.26%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:"2.1847"}),(0,s.jsx)(n.td,{children:"2.5123"}),(0,s.jsx)(n.td,{children:"61.34%"}),(0,s.jsx)(n.td,{children:"58.72%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"20"}),(0,s.jsx)(n.td,{children:"1.4562"}),(0,s.jsx)(n.td,{children:"1.8934"}),(0,s.jsx)(n.td,{children:"74.28%"}),(0,s.jsx)(n.td,{children:"68.45%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Final"}),(0,s.jsx)(n.td,{children:"0.8234"}),(0,s.jsx)(n.td,{children:"1.2156"}),(0,s.jsx)(n.td,{children:"85.70%"}),(0,s.jsx)(n.td,{children:"71.23%"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Observations:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Steady improvement in both training and validation metrics"}),"\n",(0,s.jsx)(n.li,{children:"Gap between training and validation indicates some overfitting"}),"\n",(0,s.jsxs)(n.li,{children:["Final test accuracy: ",(0,s.jsx)(n.strong,{children:"71.23%"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"evaluation-metrics-1",children:"Evaluation Metrics"}),"\n",(0,s.jsx)(n.h4,{id:"1-classification-accuracy",children:"1. Classification Accuracy"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Test Set Performance:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accuracy:"})," 71.23%"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Precision (weighted):"})," 70.15%"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Recall (weighted):"})," 71.23%"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"F1-Score (weighted):"})," 70.68%"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-loss-curves-analysis",children:"2. Loss Curves Analysis"}),"\n",(0,s.jsx)(n.p,{children:"The training shows typical convergence behavior:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Training loss decreases from 6.35 to 0.82"}),"\n",(0,s.jsx)(n.li,{children:"Validation loss decreases from 5.84 to 1.22"}),"\n",(0,s.jsx)(n.li,{children:"Divergence after epoch 15 indicates overfitting"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mitigation Strategies:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Increased dropout rates"}),"\n",(0,s.jsx)(n.li,{children:"More aggressive data augmentation"}),"\n",(0,s.jsx)(n.li,{children:"L2 regularization on dense layers"}),"\n",(0,s.jsx)(n.li,{children:"Earlier stopping criterion"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-details-1",children:"Implementation Details"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inference Process:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant U as User\n    participant M as Microphone\n    participant P as Preprocessor\n    participant Model as GRU Model\n    participant D as Decision\n    \n    U->>M: Speak (3 seconds)\n    M->>P: Raw audio\n    P->>P: Resample to 16kHz\n    P->>P: Extract Mel spectrogram\n    P->>P: Normalize features\n    P->>Model: 128x94 spectrogram\n    Model->>Model: GRU processing\n    Model->>Model: Attention weighting\n    Model->>D: Speaker probabilities\n    D->>D: Compare with threshold\n    D--\x3e>U: Verified/Rejected"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Verification Algorithm:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def verify_voice(audio_sample, user_id, threshold=0.85):\n    """\n    Verify speaker identity from voice sample\n    \n    Args:\n        audio_sample: 3-second audio recording\n        user_id: Claimed identity\n        threshold: Minimum probability for acceptance\n    \n    Returns:\n        is_verified: Boolean indicating match\n        confidence: Model\'s confidence score\n    """\n    spectrogram = preprocess_audio(audio_sample)\n    probabilities = model.predict(spectrogram)\n    confidence = probabilities[user_id]\n    is_verified = confidence >= threshold\n    return is_verified, confidence\n'})}),"\n",(0,s.jsx)(n.h3,{id:"challenges-and-future-improvements",children:"Challenges and Future Improvements"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Current Limitations:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Background Noise"}),": Performance degrades in noisy environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice Variations"}),": Illness, emotion, or fatigue affect accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Channel Effects"}),": Different microphones produce different spectrograms"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Imbalance"}),": Some speakers have more training samples"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Proposed Enhancements:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Noise Robustness"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement denoising autoencoders"}),"\n",(0,s.jsx)(n.li,{children:"Train with noise-augmented data"}),"\n",(0,s.jsx)(n.li,{children:"Use spectral subtraction preprocessing"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Domain Adaptation"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fine-tune on target device/environment"}),"\n",(0,s.jsx)(n.li,{children:"Use transfer learning from larger datasets"}),"\n",(0,s.jsx)(n.li,{children:"Implement adaptive normalization"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Optimization"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Knowledge distillation for edge deployment"}),"\n",(0,s.jsx)(n.li,{children:"Quantization for faster inference"}),"\n",(0,s.jsx)(n.li,{children:"Pruning redundant connections"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-Modal Integration"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combine with face recognition for higher confidence"}),"\n",(0,s.jsx)(n.li,{children:"Use text-dependent prompts for added security"}),"\n",(0,s.jsx)(n.li,{children:"Incorporate behavioral patterns (speaking rate, pauses)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"performance-comparison-pre-trained-vs-custom-models",children:"Performance Comparison: Pre-trained vs Custom Models"}),"\n",(0,s.jsx)(n.h3,{id:"face-recognition",children:"Face Recognition"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"FaceNet (Pre-trained)"}),(0,s.jsx)(n.th,{children:"Custom MobileNetV2"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Accuracy"}),(0,s.jsx)(n.td,{children:"95.4%"}),(0,s.jsx)(n.td,{children:"88.2%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Precision"}),(0,s.jsx)(n.td,{children:"94.8%"}),(0,s.jsx)(n.td,{children:"87.5%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Recall"}),(0,s.jsx)(n.td,{children:"93.6%"}),(0,s.jsx)(n.td,{children:"86.1%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"F1-Score"}),(0,s.jsx)(n.td,{children:"94.2%"}),(0,s.jsx)(n.td,{children:"86.8%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"EER"}),(0,s.jsx)(n.td,{children:"2.80%"}),(0,s.jsx)(n.td,{children:"4.70%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Inference Time"}),(0,s.jsx)(n.td,{children:"120ms"}),(0,s.jsx)(n.td,{children:"150ms"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"voice-recognition",children:"Voice Recognition"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"ECAPA-TDNN (Pre-trained)"}),(0,s.jsx)(n.th,{children:"Custom GRU"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Accuracy"}),(0,s.jsx)(n.td,{children:"96.8%"}),(0,s.jsx)(n.td,{children:"85.7%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Precision"}),(0,s.jsx)(n.td,{children:"96.3%"}),(0,s.jsx)(n.td,{children:"84.3%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Recall"}),(0,s.jsx)(n.td,{children:"95.9%"}),(0,s.jsx)(n.td,{children:"82.9%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"F1-Score"}),(0,s.jsx)(n.td,{children:"96.1%"}),(0,s.jsx)(n.td,{children:"83.6%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"EER"}),(0,s.jsx)(n.td,{children:"2.30%"}),(0,s.jsx)(n.td,{children:"5.10%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Inference Time"}),(0,s.jsx)(n.td,{children:"130ms"}),(0,s.jsx)(n.td,{children:"160ms"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Analysis:"})}),"\n",(0,s.jsx)(n.p,{children:"Pre-trained models show superior performance due to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Training on larger, more diverse datasets"}),"\n",(0,s.jsx)(n.li,{children:"Extensive hyperparameter tuning"}),"\n",(0,s.jsx)(n.li,{children:"Optimized architectures from research"}),"\n",(0,s.jsx)(n.li,{children:"Multiple iterations of refinement"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Custom models are competitive and offer:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adaptability to specific use cases"}),"\n",(0,s.jsx)(n.li,{children:"Lower deployment complexity"}),"\n",(0,s.jsx)(n.li,{children:"Potential for fine-tuning on domain-specific data"}),"\n",(0,s.jsx)(n.li,{children:"Full control over architecture and training"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next Section:"})," ",(0,s.jsx)(n.a,{href:"#",children:"Biometric Models - Part 2: Behavioral Authentication \u2192"})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Previous Section:"})," ",(0,s.jsx)(n.a,{href:"#",children:"\u2190 System Architecture & Overview"})]})]})}function o(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);