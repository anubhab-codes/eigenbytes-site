"use strict";(globalThis.webpackChunkeigenbytes=globalThis.webpackChunkeigenbytes||[]).push([[9643],{7046(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>o,frontMatter:()=>l,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"technical-papers/continuous-authentication/05 biometric-models-part2","title":"Biometric Models: Behavioral","description":"Overview","source":"@site/docs/publications/technical-papers/continuous-authentication/05 biometric-models-part2.md","sourceDirName":"technical-papers/continuous-authentication","slug":"/technical-papers/continuous-authentication/05 biometric-models-part2","permalink":"/publications/technical-papers/continuous-authentication/05 biometric-models-part2","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Biometric Models: Behavioral","section":"Continuous Authentication","type":"Paper Section"},"sidebar":"sidebar","previous":{"title":"Biometric Models: Physiological","permalink":"/publications/technical-papers/continuous-authentication/04 biometric-models-part1"}}');var r=i(4848),t=i(8453);const l={title:"Biometric Models: Behavioral",section:"Continuous Authentication",type:"Paper Section"},d="Biometric Models: Behavioral Authentication",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Keystroke Dynamics: LSTM-Based Authentication",id:"keystroke-dynamics-lstm-based-authentication",level:2},{value:"Introduction",id:"introduction",level:3},{value:"Keystroke Features",id:"keystroke-features",level:3},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Why LSTM for Keystroke Dynamics?",id:"why-lstm-for-keystroke-dynamics",level:3},{value:"Data Preprocessing",id:"data-preprocessing",level:3},{value:"Training Pipeline",id:"training-pipeline",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3},{value:"1. Classification Accuracy",id:"1-classification-accuracy",level:4},{value:"2. Confusion Matrix Analysis",id:"2-confusion-matrix-analysis",level:4},{value:"3. ROC-AUC Analysis",id:"3-roc-auc-analysis",level:4},{value:"4. Learning Curves",id:"4-learning-curves",level:4},{value:"Implementation Details",id:"implementation-details",level:3},{value:"Challenges and Variability",id:"challenges-and-variability",level:3},{value:"Human Activity Recognition: CNN-GRU Hybrid",id:"human-activity-recognition-cnn-gru-hybrid",level:2},{value:"Introduction",id:"introduction-1",level:3},{value:"Activity Types",id:"activity-types",level:3},{value:"Sensor Data Sources",id:"sensor-data-sources",level:3},{value:"Feature Engineering",id:"feature-engineering",level:3},{value:"Model Architecture",id:"model-architecture-1",level:3},{value:"Why CNN-GRU Hybrid?",id:"why-cnn-gru-hybrid",level:3},{value:"Training Pipeline",id:"training-pipeline-1",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics-1",level:3},{value:"1. Classification Performance",id:"1-classification-performance",level:4},{value:"2. Confusion Matrix",id:"2-confusion-matrix",level:4},{value:"3. Activity-Specific Insights",id:"3-activity-specific-insights",level:4},{value:"Implementation Details",id:"implementation-details-1",level:3},{value:"Challenges and Limitations",id:"challenges-and-limitations",level:3},{value:"Future Enhancements",id:"future-enhancements",level:3},{value:"Integration with Risk Classification",id:"integration-with-risk-classification",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"biometric-models-behavioral-authentication",children:"Biometric Models: Behavioral Authentication"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This section details the behavioral biometric models that enable continuous, passive authentication throughout user sessions. Unlike physiological biometrics (face, voice) that require active user participation, behavioral biometrics monitor user patterns during normal interaction with the system. These models analyze keystroke dynamics and human activity patterns to build a comprehensive behavioral profile."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"keystroke-dynamics-lstm-based-authentication",children:"Keystroke Dynamics: LSTM-Based Authentication"}),"\n",(0,r.jsx)(n.h3,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Keystroke dynamics analyzes the unique typing patterns of individuals, including the timing between keystrokes and how long keys are held down. This behavioral biometric operates continuously in the background, providing non-intrusive authentication without disrupting user workflow."}),"\n",(0,r.jsx)(n.h3,{id:"keystroke-features",children:"Keystroke Features"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[User Types Key Sequence] --\x3e B[Capture Events]\n    B --\x3e C[Key Press Event]\n    B --\x3e D[Key Release Event]\n    \n    C --\x3e E[Calculate Hold Time]\n    D --\x3e E\n    E --\x3e F[Dwell Time<br/>Press to Release]\n    \n    C --\x3e G[Calculate Flight Time]\n    D --\x3e G\n    G --\x3e H[Latency<br/>Release to Next Press]\n    \n    F --\x3e I[Feature Vector]\n    H --\x3e I\n    \n    style A fill:#e8f4f8\n    style I fill:#a8dadc"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Timing Features:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Feature Type"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Formula"}),(0,r.jsx)(n.th,{children:"Uniqueness"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Hold Time (Dwell)"}),(0,r.jsx)(n.td,{children:"Duration key is pressed"}),(0,r.jsx)(n.td,{children:"Release_time - Press_time"}),(0,r.jsx)(n.td,{children:"Typing pressure and habit"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Flight Time"}),(0,r.jsx)(n.td,{children:"Time between consecutive keys"}),(0,r.jsx)(n.td,{children:"Next_press - Current_release"}),(0,r.jsx)(n.td,{children:"Typing rhythm"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Digraph Latency"}),(0,r.jsx)(n.td,{children:"Time for specific key pairs"}),(0,r.jsx)(n.td,{children:"Duration between two specific keys"}),(0,r.jsx)(n.td,{children:"Finger movement patterns"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Down-Down (DD)"}),(0,r.jsx)(n.td,{children:"Press to next press"}),(0,r.jsx)(n.td,{children:"Next_press - Current_press"}),(0,r.jsx)(n.td,{children:"Overall typing speed"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Up-Up (UU)"}),(0,r.jsx)(n.td,{children:"Release to next release"}),(0,r.jsx)(n.td,{children:"Next_release - Current_release"}),(0,r.jsx)(n.td,{children:"Key release patterns"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example Digraph Features:"}),'\nFor typing "AB":']}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"DU.A.A: Hold time for key A"}),"\n",(0,r.jsx)(n.li,{children:"DU.B.B: Hold time for key B"}),"\n",(0,r.jsx)(n.li,{children:"DD.A.B: Time from press A to press B"}),"\n",(0,r.jsx)(n.li,{children:"UD.A.B: Time from release A to press B"}),"\n",(0,r.jsx)(n.li,{children:"UU.A.B: Time from release A to release B"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Input Sequence<br/>Hold & Flight Times<br/>Shape: time_steps x features] --\x3e B[LSTM Layer 1<br/>128 units<br/>return_sequences=True]\n    B --\x3e C[Dropout 30%]\n    C --\x3e D[LSTM Layer 2<br/>64 units<br/>return_sequences=False]\n    D --\x3e E[Dropout 30%]\n    E --\x3e F[Dense Layer<br/>32 units<br/>ReLU Activation]\n    F --\x3e G[Output Layer<br/>SoftMax<br/>51 users]\n    \n    style A fill:#e8f4f8\n    style G fill:#a8dadc"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Architecture Summary:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Layer"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Output Shape"}),(0,r.jsx)(n.th,{children:"Parameters"}),(0,r.jsx)(n.th,{children:"Purpose"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Input"}),(0,r.jsx)(n.td,{children:"Time Series"}),(0,r.jsx)(n.td,{children:"(31, 128)"}),(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Hold/flight time sequences"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LSTM-1"}),(0,r.jsx)(n.td,{children:"Recurrent"}),(0,r.jsx)(n.td,{children:"(31, 128)"}),(0,r.jsx)(n.td,{children:"66,560"}),(0,r.jsx)(n.td,{children:"Capture short-term typing patterns"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Dropout-1"}),(0,r.jsx)(n.td,{children:"Regularization"}),(0,r.jsx)(n.td,{children:"(31, 128)"}),(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Prevent overfitting"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LSTM-2"}),(0,r.jsx)(n.td,{children:"Recurrent"}),(0,r.jsx)(n.td,{children:"(64,)"}),(0,r.jsx)(n.td,{children:"49,408"}),(0,r.jsx)(n.td,{children:"Learn long-term typing habits"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Dropout-2"}),(0,r.jsx)(n.td,{children:"Regularization"}),(0,r.jsx)(n.td,{children:"(64,)"}),(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Prevent overfitting"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Dense"}),(0,r.jsx)(n.td,{children:"Fully Connected"}),(0,r.jsx)(n.td,{children:"(32,)"}),(0,r.jsx)(n.td,{children:"2,080"}),(0,r.jsx)(n.td,{children:"Feature compression"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Output"}),(0,r.jsx)(n.td,{children:"SoftMax"}),(0,r.jsx)(n.td,{children:"(51,)"}),(0,r.jsx)(n.td,{children:"1,683"}),(0,r.jsx)(n.td,{children:"User classification"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Total Parameters:"})," 119,731"]}),"\n",(0,r.jsx)(n.h3,{id:"why-lstm-for-keystroke-dynamics",children:"Why LSTM for Keystroke Dynamics?"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Design Rationale:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sequential Nature"}),": Typing is inherently sequential; order and timing matter"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Long-term Dependencies"}),": LSTM remembers typing habits across entire sequences"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Temporal Patterns"}),": Captures both immediate rhythm and sustained typing style"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variable Length"}),": Handles different text lengths effectively"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"LSTM Advantages:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph LR\n    A[Traditional ML] --\x3e|Fixed Features| B[Limited Context]\n    C[LSTM] --\x3e|Sequential Learning| D[Full Context]\n    \n    B --\x3e E[Misses Patterns]\n    D --\x3e F[Captures Rhythm]\n    \n    style E fill:#ff6b6b\n    style F fill:#a8dadc"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Traditional ML"}),(0,r.jsx)(n.th,{children:"LSTM"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Feature Engineering"}),(0,r.jsx)(n.td,{children:"Manual extraction required"}),(0,r.jsx)(n.td,{children:"Automatic from sequences"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Context Window"}),(0,r.jsx)(n.td,{children:"Fixed, limited"}),(0,r.jsx)(n.td,{children:"Adaptive, full sequence"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Temporal Dependencies"}),(0,r.jsx)(n.td,{children:"Weak"}),(0,r.jsx)(n.td,{children:"Strong"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Typing Rhythm"}),(0,r.jsx)(n.td,{children:"Partial capture"}),(0,r.jsx)(n.td,{children:"Complete capture"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"User Adaptability"}),(0,r.jsx)(n.td,{children:"Static"}),(0,r.jsx)(n.td,{children:"Dynamic over time"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"data-preprocessing",children:"Data Preprocessing"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Dataset:"})," CMU DSL-StrongPasswordData"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Users:"})," 51 individuals"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sessions:"})," Multiple typing sessions per user"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Password:"})," Same password typed by all users"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Features:"})," Hold times and flight times for all key combinations"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Preprocessing Pipeline:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph LR\n    A[Raw Keystroke Data] --\x3e B[Extract Timings]\n    B --\x3e C[Calculate Hold Times]\n    B --\x3e D[Calculate Flight Times]\n    C --\x3e E[Combine Features]\n    D --\x3e E\n    E --\x3e F[Normalize Using<br/>StandardScaler]\n    F --\x3e G[Create Sequences<br/>Sliding Window]\n    G --\x3e H[Train/Test Split<br/>80/20]\n    \n    style A fill:#e8f4f8\n    style H fill:#a8dadc"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Processing Steps:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Feature Extraction"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Hold time: Key press duration for each key"}),"\n",(0,r.jsx)(n.li,{children:"Flight time: Time between consecutive key releases and presses"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Normalization"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"StandardScaler: Mean = 0, Std = 1"}),"\n",(0,r.jsx)(n.li,{children:"Prevents bias toward features with larger scales"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sequence Creation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Sliding window approach for temporal context"}),"\n",(0,r.jsx)(n.li,{children:"Overlapping sequences for better coverage"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Label Encoding"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Convert user IDs to numerical labels (0-50)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"training-pipeline",children:"Training Pipeline"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Training Configuration:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Value"}),(0,r.jsx)(n.th,{children:"Rationale"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Batch Size"}),(0,r.jsx)(n.td,{children:"32"}),(0,r.jsx)(n.td,{children:"Balance between speed and stability"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Epochs"}),(0,r.jsx)(n.td,{children:"50"}),(0,r.jsx)(n.td,{children:"With early stopping"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Learning Rate"}),(0,r.jsx)(n.td,{children:"1e-3"}),(0,r.jsx)(n.td,{children:"Standard for Adam optimizer"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Optimizer"}),(0,r.jsx)(n.td,{children:"Adam"}),(0,r.jsx)(n.td,{children:"Adaptive learning rate"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Loss Function"}),(0,r.jsx)(n.td,{children:"Sparse Categorical Crossentropy"}),(0,r.jsx)(n.td,{children:"Multi-class classification"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Validation Split"}),(0,r.jsx)(n.td,{children:"20%"}),(0,r.jsx)(n.td,{children:"Monitor generalization"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Training Callbacks:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"EarlyStopping"}),": Monitor validation loss, patience of 10 epochs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ReduceLROnPlateau"}),": Reduce learning rate by 0.5 when loss plateaus for 5 epochs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ModelCheckpoint"}),": Save best model based on validation accuracy"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Training Progress:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Epoch"}),(0,r.jsx)(n.th,{children:"Training Accuracy"}),(0,r.jsx)(n.th,{children:"Training Loss"}),(0,r.jsx)(n.th,{children:"Validation Accuracy"}),(0,r.jsx)(n.th,{children:"Validation Loss"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1"}),(0,r.jsx)(n.td,{children:"6.22%"}),(0,r.jsx)(n.td,{children:"3.5714"}),(0,r.jsx)(n.td,{children:"15.01%"}),(0,r.jsx)(n.td,{children:"2.9623"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"2"}),(0,r.jsx)(n.td,{children:"15.74%"}),(0,r.jsx)(n.td,{children:"2.9303"}),(0,r.jsx)(n.td,{children:"25.25%"}),(0,r.jsx)(n.td,{children:"2.6214"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"3"}),(0,r.jsx)(n.td,{children:"26.13%"}),(0,r.jsx)(n.td,{children:"2.5719"}),(0,r.jsx)(n.td,{children:"36.27%"}),(0,r.jsx)(n.td,{children:"2.1881"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4"}),(0,r.jsx)(n.td,{children:"36.13%"}),(0,r.jsx)(n.td,{children:"2.1787"}),(0,r.jsx)(n.td,{children:"44.79%"}),(0,r.jsx)(n.td,{children:"1.8937"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"5"}),(0,r.jsx)(n.td,{children:"43.85%"}),(0,r.jsx)(n.td,{children:"1.9134"}),(0,r.jsx)(n.td,{children:"49.45%"}),(0,r.jsx)(n.td,{children:"1.7391"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"6"}),(0,r.jsx)(n.td,{children:"47.72%"}),(0,r.jsx)(n.td,{children:"1.7864"}),(0,r.jsx)(n.td,{children:"54.53%"}),(0,r.jsx)(n.td,{children:"1.5605"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Final"}),(0,r.jsx)(n.td,{children:"83.00%"}),(0,r.jsx)(n.td,{children:"0.6234"}),(0,r.jsx)(n.td,{children:"83.00%"}),(0,r.jsx)(n.td,{children:"0.6891"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,r.jsx)(n.h4,{id:"1-classification-accuracy",children:"1. Classification Accuracy"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Test Set Performance:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy:"})," 83.00%"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Precision (weighted):"})," 82.50%"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recall (weighted):"})," 83.00%"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"F1-Score (weighted):"})," 82.75%"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"2-confusion-matrix-analysis",children:"2. Confusion Matrix Analysis"}),"\n",(0,r.jsx)(n.p,{children:"The confusion matrix reveals:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Strong diagonal values indicating correct classifications"}),"\n",(0,r.jsx)(n.li,{children:"Minimal confusion between most users"}),"\n",(0,r.jsx)(n.li,{children:"Some overlap for users with similar typing patterns"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Interpretation:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"High diagonal values: Model correctly identifies most users"}),"\n",(0,r.jsx)(n.li,{children:"Off-diagonal values: Occasional misclassification (expected for similar typing styles)"}),"\n",(0,r.jsx)(n.li,{children:"Overall pattern: Clear separation between users"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"3-roc-auc-analysis",children:"3. ROC-AUC Analysis"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Per-Class Performance:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Mean AUC across all users: 0.92"}),"\n",(0,r.jsx)(n.li,{children:"Best performing users: AUC > 0.95"}),"\n",(0,r.jsx)(n.li,{children:"Challenging users: AUC between 0.85-0.90"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The ROC curves demonstrate strong discriminative ability across different thresholds, with most classes showing excellent separation."}),"\n",(0,r.jsx)(n.h4,{id:"4-learning-curves",children:"4. Learning Curves"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph LR\n    A[Epoch 1<br/>Acc: 6.22%] --\x3e B[Epoch 5<br/>Acc: 43.85%]\n    B --\x3e C[Epoch 10<br/>Acc: 62.34%]\n    C --\x3e D[Epoch 20<br/>Acc: 76.18%]\n    D --\x3e E[Final<br/>Acc: 83.00%]\n    \n    style A fill:#ff6b6b\n    style E fill:#a8dadc"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Observations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Rapid initial improvement (first 10 epochs)"}),"\n",(0,r.jsx)(n.li,{children:"Steady convergence toward 83% accuracy"}),"\n",(0,r.jsx)(n.li,{children:"Minimal gap between training and validation (good generalization)"}),"\n",(0,r.jsx)(n.li,{children:"No significant overfitting detected"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"implementation-details",children:"Implementation Details"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Enrollment Process:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant U as New User\n    participant S as System\n    participant M as Model\n    participant DB as Database\n    \n    U->>S: Register Account\n    S->>U: Request Typing Sample\n    U->>S: Type Sentence 10 Times\n    S->>S: Extract Features\n    S->>M: Generate Baseline Embedding\n    M->>DB: Store User Profile\n    DB--\x3e>U: Enrollment Complete"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Verification Process:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant U as User\n    participant S as System\n    participant M as LSTM Model\n    participant D as Decision\n    \n    U->>S: Types During Session\n    S->>S: Collect Keystroke Events\n    S->>S: Extract Features\n    S->>M: Input Sequence\n    M->>M: Generate Embedding\n    M->>D: User Prediction + Confidence\n    D->>D: Compare with Stored Profile\n    alt Match (High Confidence)\n        D--\x3e>S: Low Risk\n    else Partial Match\n        D--\x3e>S: Medium Risk\n    else No Match\n        D--\x3e>S: High Risk\n    end"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Verification Algorithm:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def verify_keystroke(keystroke_sequence, user_id, threshold=0.75):\n    \"\"\"\n    Verify user identity from keystroke pattern\n    \n    Args:\n        keystroke_sequence: Sequence of hold/flight times\n        user_id: Claimed identity\n        threshold: Minimum confidence for acceptance\n    \n    Returns:\n        risk_level: 'low', 'medium', or 'high'\n        confidence: Model's confidence score\n    \"\"\"\n    # Preprocess sequence\n    features = extract_features(keystroke_sequence)\n    normalized = scaler.transform(features)\n    \n    # Get prediction\n    predictions = model.predict(normalized)\n    confidence = predictions[user_id]\n    \n    # Determine risk level\n    if confidence >= threshold:\n        risk_level = 'low'\n    elif confidence >= threshold * 0.6:\n        risk_level = 'medium'\n    else:\n        risk_level = 'high'\n    \n    return risk_level, confidence\n"})}),"\n",(0,r.jsx)(n.h3,{id:"challenges-and-variability",children:"Challenges and Variability"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Intra-User Variability:"})}),"\n",(0,r.jsx)(n.p,{children:"Typing patterns can vary for the same user due to:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Factor"}),(0,r.jsx)(n.th,{children:"Impact"}),(0,r.jsx)(n.th,{children:"Mitigation"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Fatigue"}),(0,r.jsx)(n.td,{children:"Slower typing, longer hold times"}),(0,r.jsx)(n.td,{children:"Adaptive thresholds"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Stress"}),(0,r.jsx)(n.td,{children:"Irregular rhythm, more errors"}),(0,r.jsx)(n.td,{children:"Confidence scoring"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Multitasking"}),(0,r.jsx)(n.td,{children:"Interrupted sequences"}),(0,r.jsx)(n.td,{children:"Sequence filtering"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Time of Day"}),(0,r.jsx)(n.td,{children:"Morning vs evening differences"}),(0,r.jsx)(n.td,{children:"Time-aware modeling"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Device Type"}),(0,r.jsx)(n.td,{children:"Desktop vs laptop keyboard"}),(0,r.jsx)(n.td,{children:"Device-specific profiles"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Inter-User Similarity:"})}),"\n",(0,r.jsx)(n.p,{children:"Some users may have similar typing patterns:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Similar typing speed"}),"\n",(0,r.jsx)(n.li,{children:"Comparable experience level"}),"\n",(0,r.jsx)(n.li,{children:"Shared training background (e.g., same typing course)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mitigation Strategies:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Continuous Learning"}),": Update user profiles over time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Confidence Thresholds"}),": Adjust based on historical performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Factor Integration"}),": Combine with other biometrics for ambiguous cases"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context Awareness"}),": Consider device, time, and application context"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"human-activity-recognition-cnn-gru-hybrid",children:"Human Activity Recognition: CNN-GRU Hybrid"}),"\n",(0,r.jsx)(n.h3,{id:"introduction-1",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Human Activity Recognition (HAR) classifies physical activities based on sensor data from mobile devices. By monitoring activities like walking, sitting, standing, and running, the system adds an additional layer of behavioral verification that complements keystroke dynamics."}),"\n",(0,r.jsx)(n.h3,{id:"activity-types",children:"Activity Types"}),"\n",(0,r.jsx)(n.p,{children:"The system recognizes six distinct activities:"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[User Activities] --\x3e B[Walking]\n    A --\x3e C[Walking Upstairs]\n    A --\x3e D[Walking Downstairs]\n    A --\x3e E[Sitting]\n    A --\x3e F[Standing]\n    A --\x3e G[Laying]\n    \n    style A fill:#e8f4f8\n    style B fill:#a8dadc\n    style C fill:#a8dadc\n    style D fill:#a8dadc\n    style E fill:#a8dadc\n    style F fill:#a8dadc\n    style G fill:#a8dadc"}),"\n",(0,r.jsx)(n.h3,{id:"sensor-data-sources",children:"Sensor Data Sources"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mobile Sensors Utilized:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Sensor"}),(0,r.jsx)(n.th,{children:"Measurements"}),(0,r.jsx)(n.th,{children:"Sample Rate"}),(0,r.jsx)(n.th,{children:"Purpose"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Accelerometer"}),(0,r.jsx)(n.td,{children:"Linear acceleration (x, y, z)"}),(0,r.jsx)(n.td,{children:"50 Hz"}),(0,r.jsx)(n.td,{children:"Detect movement patterns"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Gyroscope"}),(0,r.jsx)(n.td,{children:"Angular velocity (x, y, z)"}),(0,r.jsx)(n.td,{children:"50 Hz"}),(0,r.jsx)(n.td,{children:"Capture rotational motion"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Total Body Acc"}),(0,r.jsx)(n.td,{children:"Total acceleration"}),(0,r.jsx)(n.td,{children:"50 Hz"}),(0,r.jsx)(n.td,{children:"Overall body dynamics"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Data Segmentation:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Window Size"}),": 2.56 seconds (128 readings at 50 Hz)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Overlap"}),": 50% (1.28 seconds)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Features per Window"}),": 561 engineered features"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"feature-engineering",children:"Feature Engineering"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Raw Sensor Data<br/>Time Series] --\x3e B[Sliding Window<br/>2.56s segments]\n    B --\x3e C[Time Domain Features]\n    B --\x3e D[Frequency Domain Features]\n    \n    C --\x3e E[Mean, Std, Min, Max]\n    C --\x3e F[Correlation, Energy]\n    C --\x3e G[Entropy, Skewness]\n    \n    D --\x3e H[FFT Coefficients]\n    D --\x3e I[Spectral Energy]\n    D --\x3e J[Frequency Bands]\n    \n    E --\x3e K[561-D Feature Vector]\n    F --\x3e K\n    G --\x3e K\n    H --\x3e K\n    I --\x3e K\n    J --\x3e K\n    \n    style A fill:#e8f4f8\n    style K fill:#a8dadc"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Feature Categories:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Domain (384 features)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Mean, standard deviation, min, max"}),"\n",(0,r.jsx)(n.li,{children:"Median absolute deviation"}),"\n",(0,r.jsx)(n.li,{children:"Interquartile range"}),"\n",(0,r.jsx)(n.li,{children:"Signal magnitude area"}),"\n",(0,r.jsx)(n.li,{children:"Energy measure"}),"\n",(0,r.jsx)(n.li,{children:"Correlation coefficients"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Frequency Domain (177 features)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"FFT coefficients"}),"\n",(0,r.jsx)(n.li,{children:"Spectral energy"}),"\n",(0,r.jsx)(n.li,{children:"Frequency domain entropy"}),"\n",(0,r.jsx)(n.li,{children:"Peak frequency"}),"\n",(0,r.jsx)(n.li,{children:"Frequency skewness and kurtosis"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"model-architecture-1",children:"Model Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Input<br/>561 Features] --\x3e B[Reshape for CNN<br/>30x19x1]\n    B --\x3e C[Conv1D Layer 1<br/>64 filters, kernel=3]\n    C --\x3e D[Conv1D Layer 2<br/>64 filters, kernel=3]\n    D --\x3e E[MaxPooling1D<br/>pool_size=2]\n    E --\x3e F[Flatten<br/>896 features]\n    F --\x3e G[LSTM Layer<br/>32 units]\n    G --\x3e H[Dropout 30%]\n    H --\x3e I[Dense Layer<br/>32 units, ReLU]\n    I --\x3e J[Output Layer<br/>6 classes, Sigmoid]\n    \n    style A fill:#e8f4f8\n    style J fill:#a8dadc"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Architecture Summary:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Layer"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Output Shape"}),(0,r.jsx)(n.th,{children:"Parameters"}),(0,r.jsx)(n.th,{children:"Purpose"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Input"}),(0,r.jsx)(n.td,{children:"Features"}),(0,r.jsx)(n.td,{children:"(30, 19, 1)"}),(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Reshaped sensor features"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Conv1D-1"}),(0,r.jsx)(n.td,{children:"Convolutional"}),(0,r.jsx)(n.td,{children:"(28, 64)"}),(0,r.jsx)(n.td,{children:"256"}),(0,r.jsx)(n.td,{children:"Local pattern extraction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Conv1D-2"}),(0,r.jsx)(n.td,{children:"Convolutional"}),(0,r.jsx)(n.td,{children:"(26, 64)"}),(0,r.jsx)(n.td,{children:"12,352"}),(0,r.jsx)(n.td,{children:"Hierarchical features"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MaxPooling"}),(0,r.jsx)(n.td,{children:"Pooling"}),(0,r.jsx)(n.td,{children:"(13, 64)"}),(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Dimensionality reduction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Flatten"}),(0,r.jsx)(n.td,{children:"Reshape"}),(0,r.jsx)(n.td,{children:"(832,)"}),(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Prepare for LSTM"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LSTM"}),(0,r.jsx)(n.td,{children:"Recurrent"}),(0,r.jsx)(n.td,{children:"(32,)"}),(0,r.jsx)(n.td,{children:"110,720"}),(0,r.jsx)(n.td,{children:"Temporal dependencies"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Dropout"}),(0,r.jsx)(n.td,{children:"Regularization"}),(0,r.jsx)(n.td,{children:"(32,)"}),(0,r.jsx)(n.td,{children:"0"}),(0,r.jsx)(n.td,{children:"Prevent overfitting"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Dense-1"}),(0,r.jsx)(n.td,{children:"Fully Connected"}),(0,r.jsx)(n.td,{children:"(32,)"}),(0,r.jsx)(n.td,{children:"1,056"}),(0,r.jsx)(n.td,{children:"Feature refinement"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Output"}),(0,r.jsx)(n.td,{children:"Sigmoid"}),(0,r.jsx)(n.td,{children:"(6,)"}),(0,r.jsx)(n.td,{children:"198"}),(0,r.jsx)(n.td,{children:"Activity classification"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Total Parameters:"})," 124,582"]}),"\n",(0,r.jsx)(n.h3,{id:"why-cnn-gru-hybrid",children:"Why CNN-GRU Hybrid?"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Design Rationale:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph LR\n    A[CNN] --\x3e|Spatial Features| C[Combined Model]\n    B[GRU/LSTM] --\x3e|Temporal Features| C\n    C --\x3e D[Superior Performance]\n    \n    style D fill:#a8dadc"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Strength"}),(0,r.jsx)(n.th,{children:"Application"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CNN"}),(0,r.jsx)(n.td,{children:"Spatial pattern recognition"}),(0,r.jsx)(n.td,{children:"Local feature extraction from multi-axis sensors"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LSTM/GRU"}),(0,r.jsx)(n.td,{children:"Temporal sequence modeling"}),(0,r.jsx)(n.td,{children:"Capture activity transitions and duration"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Hybrid"}),(0,r.jsx)(n.td,{children:"Best of both worlds"}),(0,r.jsx)(n.td,{children:"Spatial and temporal feature integration"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Architecture Benefits:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"CNN extracts local patterns from multi-dimensional sensor data"}),"\n",(0,r.jsx)(n.li,{children:"LSTM captures temporal dependencies across time windows"}),"\n",(0,r.jsx)(n.li,{children:"Combined approach handles both spatial and temporal aspects of movement"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"training-pipeline-1",children:"Training Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Dataset:"})," UCI HAR (Human Activity Recognition)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Subjects:"})," 30 volunteers (19-48 years)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Activities:"})," 6 daily activities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Training Samples:"})," 7,352"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Test Samples:"})," 2,947"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Features:"})," 561 per sample"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Data Preprocessing:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph LR\n    A[Raw UCI HAR Data] --\x3e B[Load Train/Test Split]\n    B --\x3e C[Normalize Features<br/>Min-Max Scaling]\n    C --\x3e D[Reshape for CNN<br/>30x19x1]\n    D --\x3e E[One-Hot Encode Labels<br/>6 classes]\n    E --\x3e F[Ready for Training]\n    \n    style A fill:#e8f4f8\n    style F fill:#a8dadc"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Training Configuration:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Value"}),(0,r.jsx)(n.th,{children:"Rationale"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Batch Size"}),(0,r.jsx)(n.td,{children:"16"}),(0,r.jsx)(n.td,{children:"Small for better gradient estimation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Epochs"}),(0,r.jsx)(n.td,{children:"30"}),(0,r.jsx)(n.td,{children:"Sufficient convergence"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Learning Rate"}),(0,r.jsx)(n.td,{children:"1e-3"}),(0,r.jsx)(n.td,{children:"Default for RMSprop"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Optimizer"}),(0,r.jsx)(n.td,{children:"RMSprop"}),(0,r.jsx)(n.td,{children:"Good for recurrent networks"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Loss Function"}),(0,r.jsx)(n.td,{children:"Categorical Crossentropy"}),(0,r.jsx)(n.td,{children:"Multi-class classification"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Validation Split"}),(0,r.jsx)(n.td,{children:"Test set provided"}),(0,r.jsx)(n.td,{children:"Standard UCI HAR split"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"evaluation-metrics-1",children:"Evaluation Metrics"}),"\n",(0,r.jsx)(n.h4,{id:"1-classification-performance",children:"1. Classification Performance"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Test Set Results:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy:"})," 89.89%"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loss:"})," 0.656"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Per-Class Performance:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Activity"}),(0,r.jsx)(n.th,{children:"Precision"}),(0,r.jsx)(n.th,{children:"Recall"}),(0,r.jsx)(n.th,{children:"F1-Score"}),(0,r.jsx)(n.th,{children:"Support"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Walking"}),(0,r.jsx)(n.td,{children:"93.75%"}),(0,r.jsx)(n.td,{children:"93.95%"}),(0,r.jsx)(n.td,{children:"93.85%"}),(0,r.jsx)(n.td,{children:"496"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Walking Upstairs"}),(0,r.jsx)(n.td,{children:"90.89%"}),(0,r.jsx)(n.td,{children:"90.87%"}),(0,r.jsx)(n.td,{children:"90.88%"}),(0,r.jsx)(n.td,{children:"471"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Walking Downstairs"}),(0,r.jsx)(n.td,{children:"86.55%"}),(0,r.jsx)(n.td,{children:"99.52%"}),(0,r.jsx)(n.td,{children:"92.58%"}),(0,r.jsx)(n.td,{children:"420"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Sitting"}),(0,r.jsx)(n.td,{children:"85.71%"}),(0,r.jsx)(n.td,{children:"54.55%"}),(0,r.jsx)(n.td,{children:"66.67%"}),(0,r.jsx)(n.td,{children:"491"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Standing"}),(0,r.jsx)(n.td,{children:"94.75%"}),(0,r.jsx)(n.td,{children:"75.10%"}),(0,r.jsx)(n.td,{children:"83.82%"}),(0,r.jsx)(n.td,{children:"532"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Laying"}),(0,r.jsx)(n.td,{children:"95.15%"}),(0,r.jsx)(n.td,{children:"95.15%"}),(0,r.jsx)(n.td,{children:"95.15%"}),(0,r.jsx)(n.td,{children:"537"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"2-confusion-matrix",children:"2. Confusion Matrix"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"                 Predicted\n              0    1    2    3    4    5\nActual 0    465   31    0    0    0    0   (Walking)\n       1     10  428   33    0    0    0   (Walking Upstairs)\n       2      2    0  418    0    0    0   (Walking Downstairs)\n       3      0    2   48   60    0    0   (Sitting)\n       4      0    0  132  399    0    0   (Standing)\n       5      0   26    0    0    0  511   (Laying)\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Observations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Excellent performance on dynamic activities (walking, laying)"}),"\n",(0,r.jsx)(n.li,{children:"Confusion between sitting and standing (expected due to similarity)"}),"\n",(0,r.jsx)(n.li,{children:"Walking variants sometimes confused (upstairs vs downstairs)"}),"\n",(0,r.jsx)(n.li,{children:"Overall strong diagonal with minimal off-diagonal confusion"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"3-activity-specific-insights",children:"3. Activity-Specific Insights"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"High Accuracy Activities:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Laying (95.15%)"}),": Most distinct sensor pattern"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Walking (93.85%)"}),": Clear periodic motion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Walking Upstairs (90.88%)"}),": Distinct acceleration pattern"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Challenging Activities:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sitting vs Standing"}),": Similar static postures, low movement"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Walking Variants"}),": Upstairs/downstairs require subtle distinction"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"implementation-details-1",children:"Implementation Details"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Real-Time Inference:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant Phone as Mobile Device\n    participant Sensors as IMU Sensors\n    participant Buffer as Data Buffer\n    participant Model as HAR Model\n    participant System as Auth System\n    \n    loop Continuous Monitoring\n        Sensors->>Buffer: Stream Data (50 Hz)\n        Buffer->>Buffer: Accumulate 2.56s Window\n        Buffer->>Model: 561 Features\n        Model->>Model: CNN-LSTM Inference\n        Model->>System: Activity Prediction\n        System->>System: Update User Profile\n        System->>System: Compare with Baseline\n    end"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Verification Process:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def verify_activity(sensor_window, user_profile, threshold=0.80):\n    \"\"\"\n    Verify user based on activity pattern\n    \n    Args:\n        sensor_window: 561-dimensional feature vector\n        user_profile: Historical activity distribution\n        threshold: Minimum similarity for low risk\n    \n    Returns:\n        risk_level: Activity-based risk assessment\n        activity: Predicted current activity\n    \"\"\"\n    # Predict activity\n    activity_probs = model.predict(sensor_window)\n    predicted_activity = np.argmax(activity_probs)\n    confidence = activity_probs[predicted_activity]\n    \n    # Compare with user profile\n    similarity = compare_activity_distribution(\n        activity_probs, \n        user_profile\n    )\n    \n    # Assess risk\n    if similarity >= threshold:\n        risk_level = 'low'\n    elif similarity >= threshold * 0.6:\n        risk_level = 'medium'\n    else:\n        risk_level = 'high'\n    \n    return risk_level, ACTIVITIES[predicted_activity]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"challenges-and-limitations",children:"Challenges and Limitations"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Current Challenges:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Device Variability"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Different phone models have different sensor characteristics"}),"\n",(0,r.jsx)(n.li,{children:"Sensor placement varies (pocket, hand, bag)"}),"\n",(0,r.jsx)(n.li,{children:"Calibration differences between devices"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Environmental Factors"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Terrain affects walking patterns (flat vs incline)"}),"\n",(0,r.jsx)(n.li,{children:"Footwear impacts gait"}),"\n",(0,r.jsx)(n.li,{children:"Carrying items changes movement dynamics"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"User Variability"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Physical condition affects activity patterns"}),"\n",(0,r.jsx)(n.li,{children:"Age and fitness level influence movement"}),"\n",(0,r.jsx)(n.li,{children:"Temporary conditions (injury, illness) alter behavior"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mitigation Strategies:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Device Normalization"}),": Calibrate for specific device types"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Adaptive Profiles"}),": Update user baselines over time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context Awareness"}),": Consider time, location, historical patterns"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Confidence Scoring"}),": Use uncertainty for ambiguous cases"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"future-enhancements",children:"Future Enhancements"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Proposed Improvements:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Transfer Learning"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Pre-train on large public datasets"}),"\n",(0,r.jsx)(n.li,{children:"Fine-tune on user-specific data"}),"\n",(0,r.jsx)(n.li,{children:"Domain adaptation for new devices"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advanced Architectures"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Attention mechanisms for key motion segments"}),"\n",(0,r.jsx)(n.li,{children:"Bidirectional LSTM for better temporal modeling"}),"\n",(0,r.jsx)(n.li,{children:"Residual connections for deeper networks"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Extended Activities"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Add cycling, driving, running"}),"\n",(0,r.jsx)(n.li,{children:"Recognize custom user activities"}),"\n",(0,r.jsx)(n.li,{children:"Continuous vs sporadic activity distinction"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Real-Time Optimization"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Model quantization for mobile deployment"}),"\n",(0,r.jsx)(n.li,{children:"Edge computing for reduced latency"}),"\n",(0,r.jsx)(n.li,{children:"Battery-efficient inference strategies"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"integration-with-risk-classification",children:"Integration with Risk Classification"}),"\n",(0,r.jsx)(n.p,{children:"Both keystroke dynamics and human activity recognition feed into the Random Forest risk classifier:"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Keystroke LSTM] --\x3e|83% Accuracy| C[Feature Vector]\n    B[Activity CNN-GRU] --\x3e|90% Accuracy| C\n    \n    C --\x3e D[Random Forest<br/>Risk Classifier]\n    \n    D --\x3e E{Risk Level}\n    E --\x3e|Low| F[Continue Session]\n    E --\x3e|Medium| G[Voice Check]\n    E --\x3e|High| H[Face Check]\n    \n    style C fill:#e8f4f8\n    style D fill:#457b9d\n    style F fill:#a8dadc\n    style G fill:#ffd93d\n    style H fill:#ff6b6b"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Combined Behavioral Profile:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Keystroke patterns provide authentication during desk work"}),"\n",(0,r.jsx)(n.li,{children:"Activity patterns verify user during mobile usage"}),"\n",(0,r.jsx)(n.li,{children:"Together, they create comprehensive behavioral coverage"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next Section:"})," ",(0,r.jsx)(n.a,{href:"#",children:"Risk Classification & Decision Logic \u2192"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous Section:"})," ",(0,r.jsx)(n.a,{href:"#",children:"\u2190 Biometric Models - Part 1: Physiological"})]})]})}function o(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453(e,n,i){i.d(n,{R:()=>l,x:()=>d});var s=i(6540);const r={},t=s.createContext(r);function l(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);