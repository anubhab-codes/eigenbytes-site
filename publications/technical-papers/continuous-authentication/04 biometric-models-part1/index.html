<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-publications docs-version-current docs-doc-page docs-doc-id-technical-papers/continuous-authentication/04 biometric-models-part1" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Biometric Models: Physiological | AG</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://eigenbytes.com/img/og-default-compressed.png"><meta data-rh="true" name="twitter:image" content="https://eigenbytes.com/img/og-default-compressed.png"><meta data-rh="true" property="og:url" content="https://eigenbytes.com/publications/technical-papers/continuous-authentication/04 biometric-models-part1"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-publications-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-publications-current"><meta data-rh="true" property="og:title" content="Biometric Models: Physiological | AG"><meta data-rh="true" name="description" content="Overview"><meta data-rh="true" property="og:description" content="Overview"><link data-rh="true" rel="icon" href="/img/eigenbytes-favicon-dark.svg" media="(prefers-color-scheme: dark)"><link data-rh="true" rel="icon" href="/img/eigenbytes-favicon.svg"><link data-rh="true" rel="canonical" href="https://eigenbytes.com/publications/technical-papers/continuous-authentication/04 biometric-models-part1"><link data-rh="true" rel="alternate" href="https://eigenbytes.com/publications/technical-papers/continuous-authentication/04 biometric-models-part1" hreflang="en"><link data-rh="true" rel="alternate" href="https://eigenbytes.com/publications/technical-papers/continuous-authentication/04 biometric-models-part1" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Technical Papers","item":"https://eigenbytes.com/publications/technical-papers/"},{"@type":"ListItem","position":2,"name":"Deep Learning–Enabled Multimodal Continuous Authentication","item":"https://eigenbytes.com/publications/technical-papers/continuous-authentication/"},{"@type":"ListItem","position":3,"name":"Biometric Models: Physiological","item":"https://eigenbytes.com/publications/technical-papers/continuous-authentication/04 biometric-models-part1"}]}</script><link rel="stylesheet" href="/assets/css/styles.9cfd3060.css">
<script src="/assets/js/runtime~main.908bde08.js" defer="defer"></script>
<script src="/assets/js/main.084b6d0a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">Home</b></a><a class="navbar__item navbar__link" href="/engineering">Engineering</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/resume">Resume</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/anubhab-codes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><a href="https://www.linkedin.com/in/anubhab-ghosh/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-linkedin-link"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/publications/"><span title="Publications" class="linkLabel_WmDU">Publications</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/publications/technical-papers/"><span title="Technical Papers" class="categoryLinkLabel_W154">Technical Papers</span></a><button aria-label="Collapse sidebar category &#x27;Technical Papers&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/publications/technical-papers/continuous-authentication/"><span title="Deep Learning–Enabled Multimodal Continuous Authentication" class="categoryLinkLabel_W154">Deep Learning–Enabled Multimodal Continuous Authentication</span></a><button aria-label="Collapse sidebar category &#x27;Deep Learning–Enabled Multimodal Continuous Authentication&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/publications/technical-papers/continuous-authentication/01 abstract"><span title="Abstract" class="linkLabel_WmDU">Abstract</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/publications/technical-papers/continuous-authentication/02 problem-context"><span title="Problem Context" class="linkLabel_WmDU">Problem Context</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/publications/technical-papers/continuous-authentication/03 system-architecture"><span title="System Architecture" class="linkLabel_WmDU">System Architecture</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/publications/technical-papers/continuous-authentication/04 biometric-models-part1"><span title="Biometric Models: Physiological" class="linkLabel_WmDU">Biometric Models: Physiological</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/publications/technical-papers/continuous-authentication/05 biometric-models-part2"><span title="Biometric Models: Behavioral" class="linkLabel_WmDU">Biometric Models: Behavioral</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/publications/technical-papers/continuous-authentication/06 risk-classification"><span title="Risk Classification &amp; Decision Logic" class="linkLabel_WmDU">Risk Classification &amp; Decision Logic</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/publications/technical-papers/continuous-authentication/07 results-performance-analysis"><span title="Results &amp; Performance Analysis" class="linkLabel_WmDU">Results &amp; Performance Analysis</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/publications/technical-papers/continuous-authentication/08 conclusion-and-future-work"><span title="Conclusion &amp; Future Work" class="linkLabel_WmDU">Conclusion &amp; Future Work</span></a></li></ul></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/publications/technical-papers/"><span>Technical Papers</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/publications/technical-papers/continuous-authentication/"><span>Deep Learning–Enabled Multimodal Continuous Authentication</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Biometric Models: Physiological</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Biometric Models - Part 1: Physiological Authentication</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview" translate="no">​</a></h2>
<p>This section details the implementation of physiological biometric authentication models used in the continuous authentication system. These models handle Face Recognition and Voice Recognition, providing strong identity verification when behavioral biometrics indicate elevated risk levels.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="face-matcher-mobilenetv2-with-triplet-loss">Face Matcher: MobileNetV2 with Triplet Loss<a href="#face-matcher-mobilenetv2-with-triplet-loss" class="hash-link" aria-label="Direct link to Face Matcher: MobileNetV2 with Triplet Loss" title="Direct link to Face Matcher: MobileNetV2 with Triplet Loss" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h3>
<p>The Face Matcher implements a deep learning-based facial recognition system inspired by FaceNet architecture. It uses MobileNetV2 as the backbone network, optimized for computational efficiency while maintaining high accuracy. The model generates 512-dimensional facial embeddings that can be compared using Euclidean distance for identity verification.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="model-architecture">Model Architecture<a href="#model-architecture" class="hash-link" aria-label="Direct link to Model Architecture" title="Direct link to Model Architecture" translate="no">​</a></h3>
<!-- -->
<p><strong>Architecture Components:</strong></p>
<table><thead><tr><th>Layer</th><th>Type</th><th>Output Shape</th><th>Parameters</th><th>Purpose</th></tr></thead><tbody><tr><td>Input</td><td>Image</td><td>(160, 160, 3)</td><td>0</td><td>RGB facial image</td></tr><tr><td>MobileNetV2</td><td>CNN Backbone</td><td>(5, 5, 1280)</td><td>2,257,984</td><td>Feature extraction (frozen)</td></tr><tr><td>GlobalAveragePooling2D</td><td>Pooling</td><td>(1280,)</td><td>0</td><td>Spatial aggregation</td></tr><tr><td>Dense</td><td>Fully Connected</td><td>(512,)</td><td>655,872</td><td>Embedding projection</td></tr><tr><td>L2 Normalization</td><td>Lambda</td><td>(512,)</td><td>0</td><td>Unit sphere constraint</td></tr></tbody></table>
<p><strong>Total Parameters:</strong> 2,913,856 (11.12 MB)</p>
<ul>
<li class=""><strong>Trainable:</strong> 655,872 (2.50 MB)</li>
<li class=""><strong>Non-trainable:</strong> 2,257,984 (8.61 MB)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-mobilenetv2">Why MobileNetV2?<a href="#why-mobilenetv2" class="hash-link" aria-label="Direct link to Why MobileNetV2?" title="Direct link to Why MobileNetV2?" translate="no">​</a></h3>
<p><strong>Design Rationale:</strong></p>
<ol>
<li class=""><strong>Efficiency</strong>: Uses depthwise separable convolutions, reducing computational cost by 8-9x compared to standard convolutions</li>
<li class=""><strong>Performance</strong>: Maintains accuracy while being lightweight (suitable for mobile/edge deployment)</li>
<li class=""><strong>Transfer Learning</strong>: Pre-trained on ImageNet provides robust feature extraction</li>
<li class=""><strong>Real-time Processing</strong>: Inference time of approximately 150ms per image</li>
</ol>
<p><strong>Comparison with Alternatives:</strong></p>
<table><thead><tr><th>Architecture</th><th>Parameters</th><th>Inference Time</th><th>Accuracy</th><th>Use Case</th></tr></thead><tbody><tr><td>ResNet-50</td><td>23.5M</td><td>280ms</td><td>95.8%</td><td>Server-side, high accuracy</td></tr><tr><td>VGG-16</td><td>138M</td><td>450ms</td><td>94.2%</td><td>Legacy systems</td></tr><tr><td>MobileNetV2</td><td>3.4M</td><td>150ms</td><td>94.5%</td><td>Edge devices, real-time</td></tr><tr><td>EfficientNet-B0</td><td>5.3M</td><td>180ms</td><td>95.1%</td><td>Balanced performance</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="loss-function-triplet-loss">Loss Function: Triplet Loss<a href="#loss-function-triplet-loss" class="hash-link" aria-label="Direct link to Loss Function: Triplet Loss" title="Direct link to Loss Function: Triplet Loss" translate="no">​</a></h3>
<p>The model is trained using triplet loss, which learns to minimize the distance between an anchor and positive (same identity) while maximizing distance to negative (different identity) samples.</p>
<p><strong>Mathematical Formulation:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">L = max(||f(a) - f(p)||² - ||f(a) - f(n)||² + α, 0)</span><br></span></code></pre></div></div>
<p>Where:</p>
<ul>
<li class=""><code>f(a)</code> = Embedding of anchor image</li>
<li class=""><code>f(p)</code> = Embedding of positive image (same person)</li>
<li class=""><code>f(n)</code> = Embedding of negative image (different person)</li>
<li class=""><code>α</code> = Margin parameter (typically 0.2)</li>
</ul>
<!-- -->
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="training-pipeline">Training Pipeline<a href="#training-pipeline" class="hash-link" aria-label="Direct link to Training Pipeline" title="Direct link to Training Pipeline" translate="no">​</a></h3>
<p><strong>Dataset:</strong> Labeled Faces in the Wild (LFW)</p>
<ul>
<li class=""><strong>Images:</strong> 13,000 labeled images</li>
<li class=""><strong>Identities:</strong> 5,750 individuals</li>
<li class=""><strong>Variations:</strong> Different lighting, poses, expressions</li>
</ul>
<p><strong>Preprocessing Steps:</strong></p>
<ol>
<li class=""><strong>Face Detection</strong>: Extract face region from image</li>
<li class=""><strong>Resize</strong>: Scale to 160x160 pixels</li>
<li class=""><strong>Normalization</strong>: Scale pixel values to [0, 1]</li>
<li class=""><strong>Augmentation</strong>: Random flips, rotations, crops</li>
</ol>
<p><strong>Training Configuration:</strong></p>
<table><thead><tr><th>Parameter</th><th>Value</th><th>Purpose</th></tr></thead><tbody><tr><td>Batch Size</td><td>32 triplets</td><td>Stable gradient updates</td></tr><tr><td>Epochs</td><td>100</td><td>Sufficient convergence</td></tr><tr><td>Learning Rate</td><td>1e-4</td><td>Fine-tuning pre-trained model</td></tr><tr><td>Optimizer</td><td>Adam</td><td>Adaptive learning</td></tr><tr><td>Margin (α)</td><td>0.2</td><td>Separation threshold</td></tr></tbody></table>
<p><strong>Training Callbacks:</strong></p>
<ul>
<li class=""><strong>EarlyStopping</strong>: Monitors validation loss, patience of 15 epochs</li>
<li class=""><strong>ReduceLROnPlateau</strong>: Reduces learning rate by 0.5 when loss plateaus for 5 epochs</li>
<li class=""><strong>ModelCheckpoint</strong>: Saves best model based on validation loss</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="evaluation-metrics">Evaluation Metrics<a href="#evaluation-metrics" class="hash-link" aria-label="Direct link to Evaluation Metrics" title="Direct link to Evaluation Metrics" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-verification-accuracy">1. Verification Accuracy<a href="#1-verification-accuracy" class="hash-link" aria-label="Direct link to 1. Verification Accuracy" title="Direct link to 1. Verification Accuracy" translate="no">​</a></h4>
<p>Optimal threshold determined at <strong>0.5392</strong> Euclidean distance:</p>
<ul>
<li class=""><strong>Training Accuracy:</strong> 99.72%</li>
<li class=""><strong>Validation Accuracy:</strong> 99.74%</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-training-and-validation-loss">2. Training and Validation Loss<a href="#2-training-and-validation-loss" class="hash-link" aria-label="Direct link to 2. Training and Validation Loss" title="Direct link to 2. Training and Validation Loss" translate="no">​</a></h4>
<p>The model demonstrates effective learning with minimal overfitting:</p>
<!-- -->
<p><strong>Observations:</strong></p>
<ul>
<li class="">Training loss decreases steadily from 0.2045 to 0.1966</li>
<li class="">Validation loss remains stable around 0.198, indicating good generalization</li>
<li class="">No significant divergence between training and validation curves</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-roc-curve-analysis">3. ROC Curve Analysis<a href="#3-roc-curve-analysis" class="hash-link" aria-label="Direct link to 3. ROC Curve Analysis" title="Direct link to 3. ROC Curve Analysis" translate="no">​</a></h4>
<p><strong>Area Under Curve (AUC):</strong> 0.7927</p>
<p>The ROC curve demonstrates the model&#x27;s ability to distinguish between matching and non-matching pairs across various thresholds. An AUC of 0.79 indicates good discriminative power, though there is room for improvement through:</p>
<ul>
<li class="">Increased training data diversity</li>
<li class="">Enhanced data augmentation</li>
<li class="">Fine-tuning of network architecture</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="4-embedding-space-visualization-t-sne">4. Embedding Space Visualization (t-SNE)<a href="#4-embedding-space-visualization-t-sne" class="hash-link" aria-label="Direct link to 4. Embedding Space Visualization (t-SNE)" title="Direct link to 4. Embedding Space Visualization (t-SNE)" translate="no">​</a></h4>
<p>t-SNE visualization of the 512-dimensional embeddings reduced to 2D shows:</p>
<ul>
<li class="">Clear clustering of same-identity samples</li>
<li class="">Separation between different identity clusters</li>
<li class="">Some overlap at cluster boundaries (expected for challenging cases)</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="5-distance-distribution-analysis">5. Distance Distribution Analysis<a href="#5-distance-distribution-analysis" class="hash-link" aria-label="Direct link to 5. Distance Distribution Analysis" title="Direct link to 5. Distance Distribution Analysis" translate="no">​</a></h4>
<p><strong>Positive Pairs (Same Identity):</strong></p>
<ul>
<li class="">Mean distance: 0.32</li>
<li class="">Standard deviation: 0.15</li>
<li class="">Distribution: Concentrated at lower distances</li>
</ul>
<p><strong>Negative Pairs (Different Identity):</strong></p>
<ul>
<li class="">Mean distance: 0.78</li>
<li class="">Standard deviation: 0.21</li>
<li class="">Distribution: Spread across higher distances</li>
</ul>
<p><strong>Optimal Threshold:</strong> 0.5392 provides best separation between distributions</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="implementation-details">Implementation Details<a href="#implementation-details" class="hash-link" aria-label="Direct link to Implementation Details" title="Direct link to Implementation Details" translate="no">​</a></h3>
<p><strong>Inference Process:</strong></p>
<!-- -->
<p><strong>Verification Algorithm:</strong></p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">verify_face</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">live_embedding</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> stored_embedding</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> threshold</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.5392</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Verify if two face embeddings belong to same person</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Args:</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        live_embedding: 512-D vector from current image</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        stored_embedding: 512-D vector from enrolled image</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        threshold: Maximum distance for positive match</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Returns:</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        is_match: Boolean indicating if faces match</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        distance: Euclidean distance between embeddings</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    distance </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> euclidean_distance</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">live_embedding</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> stored_embedding</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    is_match </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> distance </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> threshold</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> is_match</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> distance</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="challenges-and-limitations">Challenges and Limitations<a href="#challenges-and-limitations" class="hash-link" aria-label="Direct link to Challenges and Limitations" title="Direct link to Challenges and Limitations" translate="no">​</a></h3>
<p><strong>Current Challenges:</strong></p>
<ol>
<li class=""><strong>Lighting Variations</strong>: Performance degrades in extreme lighting conditions</li>
<li class=""><strong>Pose Variations</strong>: Large angles (&gt;45°) reduce accuracy</li>
<li class=""><strong>Occlusions</strong>: Masks, glasses, or partial face coverage affect recognition</li>
<li class=""><strong>Age Progression</strong>: Long-term appearance changes may require re-enrollment</li>
</ol>
<p><strong>Mitigation Strategies:</strong></p>
<ul>
<li class="">Data augmentation with varied lighting and poses</li>
<li class="">Multi-angle enrollment during registration</li>
<li class="">Periodic re-enrollment suggestions</li>
<li class="">Confidence scoring to handle uncertain cases</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="voice-matcher-gru-with-attention-mechanism">Voice Matcher: GRU with Attention Mechanism<a href="#voice-matcher-gru-with-attention-mechanism" class="hash-link" aria-label="Direct link to Voice Matcher: GRU with Attention Mechanism" title="Direct link to Voice Matcher: GRU with Attention Mechanism" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction-1">Introduction<a href="#introduction-1" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h3>
<p>The Voice Matcher implements a custom speaker recognition system using Gated Recurrent Units (GRUs) with an attention mechanism. The model processes Mel spectrograms to capture unique vocal characteristics and creates speaker-specific voiceprints for authentication.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="model-architecture-1">Model Architecture<a href="#model-architecture-1" class="hash-link" aria-label="Direct link to Model Architecture" title="Direct link to Model Architecture" translate="no">​</a></h3>
<!-- -->
<p><strong>Architecture Summary:</strong></p>
<table><thead><tr><th>Layer</th><th>Type</th><th>Output Shape</th><th>Parameters</th><th>Purpose</th></tr></thead><tbody><tr><td>Input</td><td>Mel Spectrogram</td><td>(128, 94)</td><td>0</td><td>Frequency-time representation</td></tr><tr><td>GRU-1</td><td>Recurrent</td><td>(128, 128)</td><td>86,016</td><td>Capture short-term patterns</td></tr><tr><td>BatchNorm-1</td><td>Normalization</td><td>(128, 128)</td><td>512</td><td>Stabilize training</td></tr><tr><td>Dropout-1</td><td>Regularization</td><td>(128, 128)</td><td>0</td><td>Prevent overfitting</td></tr><tr><td>GRU-2</td><td>Recurrent</td><td>(128, 64)</td><td>37,248</td><td>Refine temporal features</td></tr><tr><td>BatchNorm-2</td><td>Normalization</td><td>(128, 64)</td><td>256</td><td>Stabilize training</td></tr><tr><td>Dropout-2</td><td>Regularization</td><td>(128, 64)</td><td>0</td><td>Prevent overfitting</td></tr><tr><td>Attention</td><td>Mechanism</td><td>(128, 64)</td><td>0</td><td>Focus on key segments</td></tr><tr><td>Add</td><td>Residual</td><td>(128, 64)</td><td>0</td><td>Skip connection</td></tr><tr><td>GRU-3</td><td>Recurrent</td><td>(64,)</td><td>24,960</td><td>Final feature extraction</td></tr><tr><td>Dropout-3</td><td>Regularization</td><td>(64,)</td><td>0</td><td>Prevent overfitting</td></tr><tr><td>Dense-1</td><td>Fully Connected</td><td>(128,)</td><td>8,320</td><td>Feature refinement</td></tr><tr><td>BatchNorm-3</td><td>Normalization</td><td>(128,)</td><td>512</td><td>Stabilize training</td></tr><tr><td>Dropout-4</td><td>Regularization</td><td>(128,)</td><td>0</td><td>Prevent overfitting</td></tr><tr><td>Dense-2</td><td>Fully Connected</td><td>(64,)</td><td>8,256</td><td>Compression</td></tr><tr><td>Output</td><td>SoftMax</td><td>(665,)</td><td>43,225</td><td>Speaker classification</td></tr></tbody></table>
<p><strong>Total Parameters:</strong> 209,305</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-gru-with-attention">Why GRU with Attention?<a href="#why-gru-with-attention" class="hash-link" aria-label="Direct link to Why GRU with Attention?" title="Direct link to Why GRU with Attention?" translate="no">​</a></h3>
<p><strong>Design Rationale:</strong></p>
<ol>
<li class=""><strong>GRU Efficiency</strong>: Fewer parameters than LSTM (simpler gating mechanism)</li>
<li class=""><strong>Temporal Modeling</strong>: Captures sequential dependencies in voice data</li>
<li class=""><strong>Attention Mechanism</strong>: Focuses on discriminative voice segments</li>
<li class=""><strong>Computational Efficiency</strong>: 25% faster than LSTM with comparable accuracy</li>
</ol>
<p><strong>GRU vs LSTM Comparison:</strong></p>
<table><thead><tr><th>Aspect</th><th>GRU</th><th>LSTM</th></tr></thead><tbody><tr><td>Gates</td><td>2 (Reset, Update)</td><td>3 (Input, Forget, Output)</td></tr><tr><td>Parameters</td><td>Fewer (faster training)</td><td>More (potentially better memory)</td></tr><tr><td>Training Speed</td><td>25% faster</td><td>Baseline</td></tr><tr><td>Memory</td><td>Simpler cell state</td><td>Separate cell and hidden state</td></tr><tr><td>Performance</td><td>Comparable on voice</td><td>Slightly better on very long sequences</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="audio-preprocessing">Audio Preprocessing<a href="#audio-preprocessing" class="hash-link" aria-label="Direct link to Audio Preprocessing" title="Direct link to Audio Preprocessing" translate="no">​</a></h3>
<p><strong>Input Processing Pipeline:</strong></p>
<!-- -->
<p><strong>Preprocessing Steps:</strong></p>
<ol>
<li class=""><strong>Resampling</strong>: Standardize to 16 kHz sample rate</li>
<li class=""><strong>Segmentation</strong>: Fixed 3-second windows (truncate or pad)</li>
<li class=""><strong>STFT</strong>: Convert time-domain to frequency-domain</li>
<li class=""><strong>Mel Filterbank</strong>: Apply 128 mel-scale filters (human perception)</li>
<li class=""><strong>Logarithmic Scaling</strong>: Convert to decibels</li>
<li class=""><strong>Normalization</strong>: Zero mean, unit variance</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="attention-mechanism">Attention Mechanism<a href="#attention-mechanism" class="hash-link" aria-label="Direct link to Attention Mechanism" title="Direct link to Attention Mechanism" translate="no">​</a></h3>
<p>The attention layer allows the model to focus on the most discriminative portions of the audio sequence.</p>
<p><strong>Attention Computation:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Attention Score = softmax(Query · Key^T / √d_k)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Output = Attention Score · Value</span><br></span></code></pre></div></div>
<!-- -->
<p><strong>Benefits:</strong></p>
<ul>
<li class="">Identifies speaker-specific voice characteristics</li>
<li class="">Handles variable-length audio inputs</li>
<li class="">Improves interpretability (which parts of audio are important)</li>
<li class="">Reduces impact of background noise</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="training-pipeline-1">Training Pipeline<a href="#training-pipeline-1" class="hash-link" aria-label="Direct link to Training Pipeline" title="Direct link to Training Pipeline" translate="no">​</a></h3>
<p><strong>Dataset:</strong> Mozilla Common Voice</p>
<ul>
<li class=""><strong>Speakers:</strong> 665 unique speakers</li>
<li class=""><strong>Utterances:</strong> Multiple recordings per speaker</li>
<li class=""><strong>Duration:</strong> 3-second segments</li>
<li class=""><strong>Quality:</strong> 16 kHz, mono audio</li>
</ul>
<p><strong>Training Configuration:</strong></p>
<table><thead><tr><th>Parameter</th><th>Value</th><th>Rationale</th></tr></thead><tbody><tr><td>Batch Size</td><td>32</td><td>Memory constraints</td></tr><tr><td>Epochs</td><td>100</td><td>With early stopping</td></tr><tr><td>Learning Rate</td><td>1e-3</td><td>Initial rate for Adam</td></tr><tr><td>Optimizer</td><td>Adam</td><td>Adaptive learning rate</td></tr><tr><td>Loss Function</td><td>Sparse Categorical Crossentropy</td><td>Multi-class classification</td></tr><tr><td>Validation Split</td><td>20%</td><td>Held-out evaluation</td></tr></tbody></table>
<p><strong>Training Callbacks:</strong></p>
<ul>
<li class=""><strong>EarlyStopping</strong>: Patience of 20 epochs on validation loss</li>
<li class=""><strong>ReduceLROnPlateau</strong>: Factor of 0.5, patience of 5 epochs</li>
<li class=""><strong>ModelCheckpoint</strong>: Save best model by validation accuracy</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="training-performance">Training Performance<a href="#training-performance" class="hash-link" aria-label="Direct link to Training Performance" title="Direct link to Training Performance" translate="no">​</a></h3>
<p><strong>Loss Progression:</strong></p>
<table><thead><tr><th>Epoch</th><th>Training Loss</th><th>Validation Loss</th><th>Training Accuracy</th><th>Validation Accuracy</th></tr></thead><tbody><tr><td>1</td><td>6.3481</td><td>5.8373</td><td>1.83%</td><td>27.07%</td></tr><tr><td>2</td><td>5.3526</td><td>4.1138</td><td>25.08%</td><td>34.93%</td></tr><tr><td>3</td><td>3.9417</td><td>3.6045</td><td>34.10%</td><td>36.56%</td></tr><tr><td>4</td><td>3.4869</td><td>3.4015</td><td>38.27%</td><td>41.16%</td></tr><tr><td>5</td><td>3.2851</td><td>3.2485</td><td>42.61%</td><td>46.26%</td></tr><tr><td>10</td><td>2.1847</td><td>2.5123</td><td>61.34%</td><td>58.72%</td></tr><tr><td>20</td><td>1.4562</td><td>1.8934</td><td>74.28%</td><td>68.45%</td></tr><tr><td>Final</td><td>0.8234</td><td>1.2156</td><td>85.70%</td><td>71.23%</td></tr></tbody></table>
<p><strong>Observations:</strong></p>
<ul>
<li class="">Steady improvement in both training and validation metrics</li>
<li class="">Gap between training and validation indicates some overfitting</li>
<li class="">Final test accuracy: <strong>71.23%</strong></li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="evaluation-metrics-1">Evaluation Metrics<a href="#evaluation-metrics-1" class="hash-link" aria-label="Direct link to Evaluation Metrics" title="Direct link to Evaluation Metrics" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-classification-accuracy">1. Classification Accuracy<a href="#1-classification-accuracy" class="hash-link" aria-label="Direct link to 1. Classification Accuracy" title="Direct link to 1. Classification Accuracy" translate="no">​</a></h4>
<p><strong>Test Set Performance:</strong></p>
<ul>
<li class=""><strong>Accuracy:</strong> 71.23%</li>
<li class=""><strong>Precision (weighted):</strong> 70.15%</li>
<li class=""><strong>Recall (weighted):</strong> 71.23%</li>
<li class=""><strong>F1-Score (weighted):</strong> 70.68%</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-loss-curves-analysis">2. Loss Curves Analysis<a href="#2-loss-curves-analysis" class="hash-link" aria-label="Direct link to 2. Loss Curves Analysis" title="Direct link to 2. Loss Curves Analysis" translate="no">​</a></h4>
<p>The training shows typical convergence behavior:</p>
<ul>
<li class="">Training loss decreases from 6.35 to 0.82</li>
<li class="">Validation loss decreases from 5.84 to 1.22</li>
<li class="">Divergence after epoch 15 indicates overfitting</li>
</ul>
<p><strong>Mitigation Strategies:</strong></p>
<ul>
<li class="">Increased dropout rates</li>
<li class="">More aggressive data augmentation</li>
<li class="">L2 regularization on dense layers</li>
<li class="">Earlier stopping criterion</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="implementation-details-1">Implementation Details<a href="#implementation-details-1" class="hash-link" aria-label="Direct link to Implementation Details" title="Direct link to Implementation Details" translate="no">​</a></h3>
<p><strong>Inference Process:</strong></p>
<!-- -->
<p><strong>Verification Algorithm:</strong></p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">verify_voice</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">audio_sample</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> user_id</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> threshold</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.85</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Verify speaker identity from voice sample</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Args:</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        audio_sample: 3-second audio recording</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        user_id: Claimed identity</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        threshold: Minimum probability for acceptance</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Returns:</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        is_verified: Boolean indicating match</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        confidence: Model&#x27;s confidence score</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    spectrogram </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> preprocess_audio</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">audio_sample</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    probabilities </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">predict</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">spectrogram</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    confidence </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> probabilities</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">user_id</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    is_verified </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> confidence </span><span class="token operator" style="color:#393A34">&gt;=</span><span class="token plain"> threshold</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> is_verified</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> confidence</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="challenges-and-future-improvements">Challenges and Future Improvements<a href="#challenges-and-future-improvements" class="hash-link" aria-label="Direct link to Challenges and Future Improvements" title="Direct link to Challenges and Future Improvements" translate="no">​</a></h3>
<p><strong>Current Limitations:</strong></p>
<ol>
<li class=""><strong>Background Noise</strong>: Performance degrades in noisy environments</li>
<li class=""><strong>Voice Variations</strong>: Illness, emotion, or fatigue affect accuracy</li>
<li class=""><strong>Channel Effects</strong>: Different microphones produce different spectrograms</li>
<li class=""><strong>Data Imbalance</strong>: Some speakers have more training samples</li>
</ol>
<p><strong>Proposed Enhancements:</strong></p>
<ol>
<li class="">
<p><strong>Noise Robustness</strong>:</p>
<ul>
<li class="">Implement denoising autoencoders</li>
<li class="">Train with noise-augmented data</li>
<li class="">Use spectral subtraction preprocessing</li>
</ul>
</li>
<li class="">
<p><strong>Domain Adaptation</strong>:</p>
<ul>
<li class="">Fine-tune on target device/environment</li>
<li class="">Use transfer learning from larger datasets</li>
<li class="">Implement adaptive normalization</li>
</ul>
</li>
<li class="">
<p><strong>Model Optimization</strong>:</p>
<ul>
<li class="">Knowledge distillation for edge deployment</li>
<li class="">Quantization for faster inference</li>
<li class="">Pruning redundant connections</li>
</ul>
</li>
<li class="">
<p><strong>Multi-Modal Integration</strong>:</p>
<ul>
<li class="">Combine with face recognition for higher confidence</li>
<li class="">Use text-dependent prompts for added security</li>
<li class="">Incorporate behavioral patterns (speaking rate, pauses)</li>
</ul>
</li>
</ol>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="performance-comparison-pre-trained-vs-custom-models">Performance Comparison: Pre-trained vs Custom Models<a href="#performance-comparison-pre-trained-vs-custom-models" class="hash-link" aria-label="Direct link to Performance Comparison: Pre-trained vs Custom Models" title="Direct link to Performance Comparison: Pre-trained vs Custom Models" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="face-recognition">Face Recognition<a href="#face-recognition" class="hash-link" aria-label="Direct link to Face Recognition" title="Direct link to Face Recognition" translate="no">​</a></h3>
<table><thead><tr><th>Metric</th><th>FaceNet (Pre-trained)</th><th>Custom MobileNetV2</th></tr></thead><tbody><tr><td>Accuracy</td><td>95.4%</td><td>88.2%</td></tr><tr><td>Precision</td><td>94.8%</td><td>87.5%</td></tr><tr><td>Recall</td><td>93.6%</td><td>86.1%</td></tr><tr><td>F1-Score</td><td>94.2%</td><td>86.8%</td></tr><tr><td>EER</td><td>2.80%</td><td>4.70%</td></tr><tr><td>Inference Time</td><td>120ms</td><td>150ms</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="voice-recognition">Voice Recognition<a href="#voice-recognition" class="hash-link" aria-label="Direct link to Voice Recognition" title="Direct link to Voice Recognition" translate="no">​</a></h3>
<table><thead><tr><th>Metric</th><th>ECAPA-TDNN (Pre-trained)</th><th>Custom GRU</th></tr></thead><tbody><tr><td>Accuracy</td><td>96.8%</td><td>85.7%</td></tr><tr><td>Precision</td><td>96.3%</td><td>84.3%</td></tr><tr><td>Recall</td><td>95.9%</td><td>82.9%</td></tr><tr><td>F1-Score</td><td>96.1%</td><td>83.6%</td></tr><tr><td>EER</td><td>2.30%</td><td>5.10%</td></tr><tr><td>Inference Time</td><td>130ms</td><td>160ms</td></tr></tbody></table>
<p><strong>Analysis:</strong></p>
<p>Pre-trained models show superior performance due to:</p>
<ul>
<li class="">Training on larger, more diverse datasets</li>
<li class="">Extensive hyperparameter tuning</li>
<li class="">Optimized architectures from research</li>
<li class="">Multiple iterations of refinement</li>
</ul>
<p>Custom models are competitive and offer:</p>
<ul>
<li class="">Adaptability to specific use cases</li>
<li class="">Lower deployment complexity</li>
<li class="">Potential for fine-tuning on domain-specific data</li>
<li class="">Full control over architecture and training</li>
</ul>
<hr></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/publications/technical-papers/continuous-authentication/03 system-architecture"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">System Architecture</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/publications/technical-papers/continuous-authentication/05 biometric-models-part2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Biometric Models: Behavioral</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#face-matcher-mobilenetv2-with-triplet-loss" class="table-of-contents__link toc-highlight">Face Matcher: MobileNetV2 with Triplet Loss</a><ul><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#model-architecture" class="table-of-contents__link toc-highlight">Model Architecture</a></li><li><a href="#why-mobilenetv2" class="table-of-contents__link toc-highlight">Why MobileNetV2?</a></li><li><a href="#loss-function-triplet-loss" class="table-of-contents__link toc-highlight">Loss Function: Triplet Loss</a></li><li><a href="#training-pipeline" class="table-of-contents__link toc-highlight">Training Pipeline</a></li><li><a href="#evaluation-metrics" class="table-of-contents__link toc-highlight">Evaluation Metrics</a></li><li><a href="#implementation-details" class="table-of-contents__link toc-highlight">Implementation Details</a></li><li><a href="#challenges-and-limitations" class="table-of-contents__link toc-highlight">Challenges and Limitations</a></li></ul></li><li><a href="#voice-matcher-gru-with-attention-mechanism" class="table-of-contents__link toc-highlight">Voice Matcher: GRU with Attention Mechanism</a><ul><li><a href="#introduction-1" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#model-architecture-1" class="table-of-contents__link toc-highlight">Model Architecture</a></li><li><a href="#why-gru-with-attention" class="table-of-contents__link toc-highlight">Why GRU with Attention?</a></li><li><a href="#audio-preprocessing" class="table-of-contents__link toc-highlight">Audio Preprocessing</a></li><li><a href="#attention-mechanism" class="table-of-contents__link toc-highlight">Attention Mechanism</a></li><li><a href="#training-pipeline-1" class="table-of-contents__link toc-highlight">Training Pipeline</a></li><li><a href="#training-performance" class="table-of-contents__link toc-highlight">Training Performance</a></li><li><a href="#evaluation-metrics-1" class="table-of-contents__link toc-highlight">Evaluation Metrics</a></li><li><a href="#implementation-details-1" class="table-of-contents__link toc-highlight">Implementation Details</a></li><li><a href="#challenges-and-future-improvements" class="table-of-contents__link toc-highlight">Challenges and Future Improvements</a></li></ul></li><li><a href="#performance-comparison-pre-trained-vs-custom-models" class="table-of-contents__link toc-highlight">Performance Comparison: Pre-trained vs Custom Models</a><ul><li><a href="#face-recognition" class="table-of-contents__link toc-highlight">Face Recognition</a></li><li><a href="#voice-recognition" class="table-of-contents__link toc-highlight">Voice Recognition</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Site</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/engineering">Engineering</a></li><li class="footer__item"><a class="footer__link-item" href="/publications">Publications</a></li><li class="footer__item"><a class="footer__link-item" href="/resume">Resume</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/anubhab-codes" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/anubhab-ghosh/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://lichess.org/@/ravenclaw_x" target="_blank" rel="noopener noreferrer" class="footer__link-item">Lichess<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Anubhab Ghosh.</div></div></div></footer></div>
</body>
</html>